{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **ML Model - Predict Remaining Useful Life (RUL)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Answer [Business Requirement 1](https://github.com/roeszler/filter-maintenance-predictor/blob/main/README.md#business-requirements) :\n",
    "*   Fit and evaluate a **regression model** to predict the Remaining Useful Life of a replaceable part\n",
    "*   Fit and evaluate a **classification model** to predict the Remaining Useful Life of a replaceable part should the regressor not perform well.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Data cleaning:\n",
    "* outputs/datasets/cleaned/dfCleanTotal.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Validation set (features and target)\n",
    "* ML pipeline to predict RUL\n",
    "* A map of the labels\n",
    "* Feature Importance Plot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The major steps in this Regressor Pipeline\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 0.9rem;\"><strong>1. ML Pipeline: Regressor</strong> (Dropdown List)</summary>\n",
    "\n",
    "* Create Regressor Pipeline\n",
    "* Split the train set\n",
    "* Grid Search CV SKLearn\n",
    "    * Use standard hyperparameters to find most suitable algorithm\n",
    "    * Extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "* Assess Feature Performance\n",
    "* Evaluate Regressor\n",
    "* Create Train, Test, Validation Sets\n",
    "</details></br>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 0.9rem;\"><strong>2. ML Pipeline: Regressor + Principal Component Analysis</strong> (PCA)</summary>\n",
    "\n",
    "* Prepare the Data for the Pipeline\n",
    "* Create Regressor + PCA Pipeline\n",
    "* Split the train and validation sets\n",
    "* Grid Search CV SKLearn\n",
    "    * Use standard hyperparameters to find most suitable algorithm\n",
    "    * Do an extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "* Assess Feature Performance\n",
    "* Evaluate Regressor\n",
    "* Create Train, Test, Validation Sets\n",
    "</details></br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 0.9rem;\"><strong>3. Convert Regression to Classification</strong> (Optionally)</summary>\n",
    "\n",
    "* Convert numerical target to bins, and check if it is balanced\n",
    "* Rewrite Pipeline for ML Modelling\n",
    "* Load Algorithms For Classification\n",
    "* Split the Train Test sets:\n",
    "* Grid Search CV SKLearn:\n",
    "    * Use standard hyper parameters to find most suitable model\n",
    "    * Grid Search CV\n",
    "    * Check Result\n",
    "* Do an extensive search on the most suitable model to find the best hyperparameter configuration.\n",
    "    * Define Model Parameters\n",
    "    * Extensive Grid Search CV                             \n",
    "    * Check Results\n",
    "    * Check Best Model\n",
    "    * Parameters for best model\n",
    "    * Define the best clf_pipeline\n",
    "* Assess Feature Importance\n",
    "* Evaluate Classifier on Train and Test Sets\n",
    "    * Custom Function\n",
    "    * List that relates the classes and tenure interval\n",
    "</details></br>\n",
    "\n",
    "<details><summary style=\"font-size: 0.9rem;\"><strong>4. Decide which pipeline to use</strong></summary></details></br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 0.9rem;\"><strong>5. Refit with the best features</strong></summary>\n",
    "\n",
    "* Rewrite Pipeline\n",
    "* Split Train Test Set with only best features\n",
    "* Subset best features\n",
    "* Grid Search CV SKLearn\n",
    "* Best Parameters\n",
    "    * Manually\n",
    "* Grid Search CV\n",
    "* Check Results\n",
    "* Check Best Model\n",
    "* Define the best pipeline\n",
    "</details></br>\n",
    "\n",
    "<details><summary style=\"font-size: 0.9rem;\"><strong>6. Assess Feature Importance</strong></summary></details></br>\n",
    "\n",
    "<details><summary style=\"font-size: 0.9rem;\"><strong>7. Push Files to Repo</strong></summary></details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Modelling:\n",
    "The hypothesis part of the process where you will find out whether you can answer the question.\n",
    "* Identify what techniques to use.\n",
    "* Split your data into train, validate and test sets.\n",
    "* Build and train the models with the train data set.\n",
    "* Validate Models and hyper-parameter : Trial different machine learning methods and models with the validation data set.\n",
    "* Poor Results - return to data preparation for feature engineering\n",
    "* Successful hypothesis - where the inputs from the data set are mapped to the output target / label appropriately to evaluate.\n",
    "\n",
    "5. Evaluation:\n",
    "Where you test whether the model can predict unseen data.\n",
    "* Test Dataset\n",
    "* Choose the model that meets the business success criteria best.\n",
    "* Review and document the work that you have done.\n",
    "* If your project meets the success metrics you defined with your customer?\n",
    "- Ready to deploy. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data\n",
    "Target variable for regressor, remove from classifier and drop other variables not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "df_total = pd.read_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv') # data with all negative log_EWM values removed\n",
    "df_total_model = (pd.read_csv('outputs/datasets/transformed/dfTransformedTotal.csv')\n",
    "        .drop(labels=['4point_EWM', 'change_DP', 'change_EWM'], axis=1)\n",
    "    )\n",
    "df_train_even_dist = (pd.read_csv(f'outputs/datasets/transformed/dfTransformedTrain.csv')\n",
    "        .drop(labels=['4point_EWM', 'change_DP', 'change_EWM', 'std_DP', 'median_DP', 'bin_size'], axis=1)\n",
    "    )\n",
    "print(df_total.shape, '= df_total')\n",
    "print(df_total_model.shape, '= df_total_model')\n",
    "print(df_train_even_dist.shape, '= df_train_even_dist')\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline : Regressor\n",
    "## Create Regressor Pipeline\n",
    "### Set the Transformations\n",
    "* Smart correlation\n",
    "* feat_scaling\n",
    "* feat_selection\n",
    "* Modelling\n",
    "* Model as variable\n",
    "\n",
    "Note: Numerical Transformation not required as data supplied as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train, Test, Validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is discrete however in bins, so:\n",
    "#### Define Cleaned **Train** & **Test** Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_total_model['Data_No'].iloc[0:len(df_total)]\n",
    "df_test = df_total_model[n > 50].reset_index(drop=True)\n",
    "df_train = df_train_even_dist # smaller dataset\n",
    "# df_train = df_total_model[n < 51].reset_index(drop=True) # larger dataset\n",
    "# df_train = df_train_even_dist.fillna(0)\n",
    "\n",
    "df_train_model = df_train_even_dist\n",
    "# df_train_model = df_train_even_dist.fillna(0)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine **Target** and **Independent** Variables and Extract **Validation** Dataset\n",
    "\n",
    "As discussed in the readme, this data has been supplied pre-split into **train** and **test** within unique **data bins**. \n",
    "We extract random observations from the **test** dataset to create a **validation** set, in a 70:30 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review correlations, Drop Features and Split into **70% test** and **30% validate**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "df_engineering = df_test.copy()\n",
    "corr_sel.fit_transform(df_engineering)\n",
    "\n",
    "# log_EWM = df_test['log_EWM']\n",
    "features_to_drop_test = corr_sel.features_to_drop_\n",
    "features_to_drop_test = [e for e in features_to_drop_test if e not in ('Dust_feed', 'Time', 'RUL', 'mass_g', 'cumulative_mass_g', 'filter_balance')] # prevent these requirements from being removed in V1\n",
    "# features_to_drop_test = [e for e in features_to_drop_test if e not in ('Dust_feed', 'RUL', 'mass_g', 'cumulative_mass_g', 'filter_balance')] # prevent these requirements from being removed in V1\n",
    "# features_to_drop_test = [e for e in features_to_drop_test if e not in ('Dust_feed', 'Time', 'RUL', 'mass_g', 'cumulative_mass_g')] # prevent these requirements from being removed in V1\n",
    "features_to_drop_test.insert(0, 'Differential_pressure') # include differential pressure to be removed\n",
    "# features_to_drop_test.insert(0, 'Tt') # include differential pressure to be removed\n",
    "X = df_test.drop(features_to_drop_test,axis=1)\n",
    "y = df_test['Differential_pressure'] # define the target variable\n",
    "# y = df_test['filter_balance'] # define the target variable\n",
    "# y = df_test['Tt'] # define the target variable\n",
    "# y = df_test['Time'] # define the target variable\n",
    "\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X,y,test_size=0.30, random_state=0)\n",
    "\n",
    "print(X_test.shape, 'X_test')\n",
    "print(X_validate.shape, 'X_validate')\n",
    "print(y_test.shape, 'y_test')\n",
    "print(y_validate.shape, 'y_validate')\n",
    "print('\\nFeatures Suggested to Drop :\\n', features_to_drop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define **X_train**, **y_train** variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create **train** dataset with the same variables dropped as the **test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop_test.insert(0, 'RUL')\n",
    "\n",
    "X_train = df_train.drop(features_to_drop_test,axis=1)\n",
    "y_train = df_train['Differential_pressure']\n",
    "# y_train = df_train['filter_balance']\n",
    "# y_train = df_train['Tt']\n",
    "# y_train = df_train['Time']\n",
    "\n",
    "print(X_train.shape, 'X_train')\n",
    "print(y_train.shape, 'y_train')\n",
    "print('\\nFeatures Dropped :\\n', features_to_drop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Target Imbalance\n",
    "### No need to handle target imbalance in this **regression model**.\n",
    "* Typically we only need to create a single pipeline for Classification or Regression task. \n",
    "* The exception occurs when we need to handle a **classification target imbalance**, which requires more than one model to process "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline for **Fitting Models** (regression)\n",
    "Import features & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Management\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML regression algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        # (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "        #                                              variables=['Differential_pressure', 'Flow_rate',\n",
    "        #                                                         # 'log_EWM', 'Time', 'mass_g', 'Tt', 'filter_balance',\n",
    "        #                                                         'Dust_feed', 'Dust', 'cumulative_mass_g'])),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "        (\"feat_selection\",  SelectFromModel(model)),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    return pipeline_base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Custom Class** to fit a set of algorithms, each with its own set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineOptimization(self.models[key]) # the model\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score (R²)'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score (R²)': np.mean(scores),\n",
    "                'stdDev_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score (R²)', 'max_score', 'stdDev_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use standard hyperparameters to find most suitable algorithm for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=0),\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=0),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(random_state=0),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(random_state=0),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=0),\n",
    "    'XGBRegressor': XGBRegressor(random_state=0),\n",
    "    'SGDRegressor': SGDRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    'DecisionTreeRegressor': {},\n",
    "    'RandomForestRegressor': {},\n",
    "    'ExtraTreesRegressor': {},\n",
    "    'AdaBoostRegressor': {},\n",
    "    'GradientBoostingRegressor': {},\n",
    "    'XGBRegressor': {},\n",
    "    'SGDRegressor': {},\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the pipelines, using the above models with **default hyperparameters** to find the most suitable model\n",
    "* Parsed the train set\n",
    "* Set the performance metric as an R² score (Regression: described in our ML business case)\n",
    "* Cross validation as 5 (rule of thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_quick, grid_search_pipelines_quick = search.score_summary(sort_by='mean_score (R²)')\n",
    "grid_search_summary_quick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* The average **R² score** (mean_score) indicates how well a model of the data fits the actual data.\n",
    "    * A value of R² score = 1 represents a perfect fit and R² score = 0 indicates the model is not any better than simply estimating an average.\n",
    "\n",
    "* Our R² score ranges from **0.55 to 0.83**, which includes model performances higher than the **0.7** tolerance we decided in the business case.\n",
    "* The best result is the **Extreme Gradient Boosting Regressor** at an R² score of **0.83** and standard deviation of **0.344381**. \n",
    "* The **Linear** and **Stochastic Gradient Descent** regressors also perform above our business requirements and would be good alternates for modelling should we need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensive Grid Search \n",
    "Perform an extensive search on the most suitable model to find the best **hyperparameter configuration** with the aim to **improve performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_quick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation to help on hyperparameter list: \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "models_search = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    'XGBRegressor':{\n",
    "        'model__base_score': [0.5],\n",
    "        # 'model__booster': ['gbtree', 'gblinear'],\n",
    "        # 'model__booster': ['gbtree'],\n",
    "        'model__booster': ['gblinear'],\n",
    "        # 'model__eval_metric': ['rmse', 'mae', 'rmsle', 'map'],\n",
    "        'model__eval_metric': ['rmse'],\n",
    "        # 'model__learning_rate': [0.1, 0.3],\n",
    "        'model__learning_rate': [0.3],\n",
    "        'model__max_depth': [2, 4, 6],\n",
    "        'model__n_estimators': [10, 20, 30, 50, 100, 20931],\n",
    "        'model__objective': ['reg:squarederror'],\n",
    "        'model__verbosity': [0],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "XGB_search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_XGBR, grid_search_pipelines_XGBR = XGB_search.score_summary(sort_by='mean_score (R²)')\n",
    "grid_search_summary_XGBR.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation to help on hyperparameter list: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "lin_model_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "}\n",
    "\n",
    "lin_params_search = {\n",
    "    'LinearRegression':{\n",
    "        'model__fit_intercept': [False, True],\n",
    "        'model__positive': [False, True],\n",
    "        'model__copy_X': [True],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_search = HyperparameterOptimizationSearch(models=lin_model_search, params=lin_params_search)\n",
    "lin_search.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_linear, grid_search_pipelines_linear = lin_search.score_summary(sort_by='mean_score (R²)')\n",
    "grid_search_summary_linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation to summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary = pd.concat([grid_search_summary_XGBR, grid_search_summary_linear], ignore_index=True)\n",
    "grid_search_pipelines = dict(grid_search_pipelines_XGBR); grid_search_pipelines.update(grid_search_pipelines_linear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation to help on hyperparameter list: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "sgd_model_search = {\n",
    "    'SGDRegressor': SGDRegressor(),\n",
    "}\n",
    "\n",
    "sgd_params_search = {\n",
    "    'SGDRegressor':{\n",
    "        'model__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'model__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'model__alpha': [0.0001,0.0002,0.0003],\n",
    "        'model__learning_rate': [1e-1,1e-2,1e-3, 'optimal', 'adaptive'],\n",
    "        'model__fit_intercept': [False, True],\n",
    "        'model__early_stopping': [True],\n",
    "        'model__average': [True],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_search = HyperparameterOptimizationSearch(models=sgd_model_search, params=sgd_params_search)\n",
    "SGD_search.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_SGDR, grid_search_pipelines_SGDR = SGD_search.score_summary(sort_by='mean_score (R²)')\n",
    "grid_search_summary_SGDR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation to summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary = pd.concat([grid_search_summary, grid_search_summary_SGDR], ignore_index = True)\n",
    "data = dict(grid_search_pipelines); data.update(grid_search_pipelines_SGDR)\n",
    "grid_search_pipelines = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best **Model**, **Parameters** and **Pipeline** from Extensive Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_quick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary.iloc[:, 0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The current **best model** performance is:\n",
    "* **XGBRegressor** with \n",
    "    <!-- * R² Score at **0.827807** and -->\n",
    "    * R² Score at **0.83843** and\n",
    "    <!-- * Standard Deviation at **0.344384** -->\n",
    "    * Standard Deviation at **0.323141**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "grid_search_summary.iloc[0].dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The **parameter configuration** of this model is:\n",
    "* XGBRegressor with\n",
    "    * max_depth = **6**\n",
    "    * n_estimators = **50**\n",
    "    * Note: \n",
    "        * This can be replaced with all **n_estimators = 20931**, (seen at row index 4) with no discernable change in the performance of the R² or standard deviation Scores.\n",
    "        * A sensitivity analysis was performed by including the total data from the df_train data bin:\n",
    "            * The performance decreased to R² Score at _0.500771_ and Standard Deviation at _0.611555_\n",
    "            * This confirms that we have successfully reduced some of the noise of the data by engineering a more even distribution among **dust** type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best **Regressor Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Considerations\n",
    "Considering the value of using all data (n_estimators = 20931) to achieve the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_heuristic_model = grid_search_summary.iloc[4,:]\n",
    "best_heuristic_model.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The **test scores** and **best model parameters** for the remaining models are:\n",
    "\n",
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary_short = grid_search_summary.iloc[:, 0:5]\n",
    "grid_search_summary_short[grid_search_summary_short.estimator != grid_search_summary_short.estimator.shift(1)].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores and Parameters of each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = grid_search_summary_short[grid_search_summary_short.estimator != grid_search_summary_short.estimator.shift(1)].head()\n",
    "[print(f'========\\n', grid_search_summary.iloc[n,:].dropna()) for n in pipe.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Feature Performance\n",
    "To find the most important features in this pipeline. \n",
    "\n",
    "If the best model is tree based, we can access these features using **.features_importances**.......???\n",
    "\n",
    "<!-- * The \"best features\" information is found in the pipeline's \"feature selection\" step as a boolean list.\n",
    "* We can use this list to subset the train set columns.\n",
    "* We create a DataFrame that contains these features' importance and plot it as a bar plot. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the best model from the GridCV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_summary = {f'{best_model}': best_parameters}\n",
    "params_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include best hyperparameter values into the Hyperparameter Optimization Search function \n",
    "* The **best_parameters** dictionary is not in the correct format, so has been entered manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation to help on hyperparameter list: \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "model = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'XGBRegressor':{\n",
    "        'model__base_score': [0.5],\n",
    "        'model__booster': ['gblinear'],\n",
    "        'model__eval_metric': ['rmse'],\n",
    "        'model__learning_rate': [0.3],\n",
    "        'model__max_depth': [2],\n",
    "        'model__n_estimators': [50],\n",
    "        'model__objective': ['reg:squarederror'],\n",
    "        'model__verbosity': [0],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def PipelineOptimization(model):\n",
    "#     pipeline_base = Pipeline([\n",
    "#         # (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "#         #                                              variables=['Differential_pressure', 'Flow_rate',\n",
    "#         #                                                         # 'log_EWM', 'Time', 'mass_g', 'Tt', 'filter_balance',\n",
    "#         #                                                         'Dust_feed', 'Dust', 'cumulative_mass_g'])),\n",
    "#         ('SmartCorrelatedSelection', SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "#         ('feat_scaling', StandardScaler()),\n",
    "#         ('feat_selection',  SelectFromModel(model)),\n",
    "#         ('model', model),\n",
    "#         # ('model',\n",
    "#         #     XGBRegressor(\n",
    "#         #         base_score=0.5,\n",
    "#         #         booster='gblinear',\n",
    "#         #         eval_metric='rmse',\n",
    "#         #         max_depth=2,\n",
    "#         #         n_estimators=50,\n",
    "#         #         objective='reg:squarederror',\n",
    "#         #         verbosity = 0,)),\n",
    "#     ])\n",
    "#     return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = HyperparameterOptimizationSearch(models=model, params=params)\n",
    "optimal_model.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model_summary, optimal_model_pipeline = optimal_model.score_summary(sort_by='mean_score (R²)')\n",
    "optimal_model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xbg_reg = xgb.XGBRegressor(\n",
    "#     base_score=0.5,\n",
    "#     booster='gblinear',\n",
    "#     eval_metric='rmse',\n",
    "#     max_depth=2,\n",
    "#     n_estimators=50,\n",
    "#     objective='reg:squarederror',\n",
    "#     verbosity = 0,\n",
    "#     ).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = datasets.load_diabetes(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_test_drop = X_test.drop('RUL', axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test_drop)\n",
    "\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the regressor with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbg_reg = xgb.XGBRegressor().fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xbg_reg.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_heading = X_train.transpose().reset_index().rename(columns={'index':'Variable'})\n",
    "# v_heading = c_heading['Variable'].to_list()\n",
    "# v_heading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_df_copy = importance_df.copy()\n",
    "# importance_df_copy['Variable'] = v_heading\n",
    "# test = importance_df_copy.set_index('Variable', drop=True).rename_axis(None)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rename Axis\n",
    "f_importance = xbg_reg.get_booster().get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame.from_dict(data=f_importance, orient='index')\n",
    "c_heading = X_train.transpose().reset_index().rename(columns={'index':'Variable'})\n",
    "v_heading = c_heading['Variable'].to_list()\n",
    "importance_df['Variable'] = v_heading\n",
    "\n",
    "# Plot Most Important Feature\n",
    "plot_importance = importance_df.set_index('Variable', drop=True).rename_axis(None)\n",
    "plot_importance.plot.bar()\n",
    "\n",
    "# plt.legend()\n",
    "plt.xticks(fontsize=8)\n",
    "plt.title('Features by Importance Score\\n')\n",
    "plt.ylabel('Importance Score\\n(gain in accuracy)\\n')\n",
    "# training_score = xbg_reg.score(X_train, y_train)\n",
    "# plt.xlabel('\\nFeature\\nModel Training Score: %.4f' % training_score)\n",
    "plt.xlabel('\\nFeature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance = plot_importance.drop(['filter_balance'], axis=0)\n",
    "# plot_importance = plot_importance.drop(['Differential_pressure'], axis=0)\n",
    "plot_importance.plot.bar()\n",
    "\n",
    "# plt.legend()\n",
    "plt.xticks(fontsize=8)\n",
    "plt.title('Features by Importance Score\\n')\n",
    "plt.ylabel('Importance Score\\n(gain in accuracy)\\n')\n",
    "plt.xlabel('\\nFeature')\n",
    "# plt.xlabel('\\nFeature\\nModel Training Score: %.4f' % training_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = datasets.load_diabetes(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_test_drop = X_test.drop(['RUL', 'filter_balance'], axis=1)\n",
    "# X_test_drop = X_test.drop(['RUL', 'Differential_pressure'], axis=1)\n",
    "X_train_drop = X_train.drop('filter_balance', axis=1)\n",
    "# X_train_drop = X_train.drop('Differential_pressure', axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_drop)\n",
    "X_test_scaled = scaler.transform(X_test_drop)\n",
    "\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgbr = xgb.XGBRegressor(\n",
    "    base_score=0.5,\n",
    "    booster='gblinear',\n",
    "    eval_metric='rmse',\n",
    "    max_depth=2,\n",
    "    n_estimators=50,\n",
    "    objective='reg:squarederror',\n",
    "    verbosity = 0,\n",
    "    )\n",
    "\n",
    "xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create comparison dataframe and Visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test_drop = X_test.drop('RUL', axis=1)\n",
    "y_pred = xgbr.predict(X_test_drop)\n",
    "\n",
    "# Create Dataframe\n",
    "y_test_df = pd.DataFrame(columns=['y_test'])\n",
    "y_test_df['y_test'] = y_test\n",
    "df_pred = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "df_comp = y_test_df.reset_index(drop=True)\n",
    "df_comp['y_pred'] = df_pred.y_pred.reset_index(drop=True)\n",
    "df_comp['variance'] = ((df_comp.y_pred - df_comp.y_test)/sum(y_test,y_pred))*100\n",
    "\n",
    "# Visualize it\n",
    "sample = df_comp.sort_values(by='y_test')\n",
    "my_range = range(1,len(sample.index)+1)\n",
    "plt.scatter(sample['y_pred'], my_range, color='navy', alpha=1, label='y_pred')\n",
    "plt.scatter(sample['y_test'], my_range, color='gold', alpha=0.8 , label='y_test')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(fontsize=8)\n",
    "plt.title('Snapshot of the dataset tail showing variation between Actual\\nand Predicted measures in Differential Pressure\\n')\n",
    "plt.ylabel('Index No.\\n')\n",
    "plt.xlabel('\\nPredicted Differential Pressure vs Actual\\nAverage Variation (Pa): %.4f' % df_comp.variance.mean())\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Regressor, Train and Test Set Performance\n",
    "Compute a performance metric on the data held out for testing, **df_test**\n",
    "* [R² score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) (also called Coefficient of Determination)\n",
    "* [Mean Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE)\n",
    "* [Median Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) (MdAE)\n",
    "* [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE)\n",
    "* Root Mean Squared Error (RMSE).\n",
    "\n",
    "We could also consider:\n",
    "* Almost Correct Predictions Error Rate (ACPER)\n",
    "* Mean Absolute Percentage Error (MAPE) and \n",
    "* Adjusted R² Score \n",
    "    * _((1 - R²) * (sample_size - 1)) * -1 / (sample_size - no_independent_features - 1))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model_summary.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* R2 Score: -0.107\n",
    "* Mean Absolute Error: 97.403\n",
    "* Median Absolute Error: 85.597\n",
    "* Mean Squared Error: 16560.575\n",
    "* Root Mean Squared Error: 128.688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    # sample_size = len(X)\n",
    "    # no_indep_features = y.shape[1]\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    # print('Adjusted R2 Score:', -(1 - R2)*(sample_size - 1)/(sample_size - no_indep_features - 1))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print('Median Absolute Error:', median_absolute_error(y, prediction).round(3))\n",
    "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
    "    print('Root Mean Squared Error:', np.sqrt(\n",
    "        mean_squared_error(y, prediction)).round(3))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test_drop, y_test, best_regressor_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    # sample_size = len(X)\n",
    "    # no_indep_features = y.shape[1]\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    # print('Adjusted R2 Score:', -(1 - R2)*(sample_size - 1)/(sample_size - no_indep_features - 1))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print('Median Absolute Error:', median_absolute_error(y, prediction).round(3))\n",
    "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
    "    print('Root Mean Squared Error:', np.sqrt(\n",
    "        mean_squared_error(y, prediction)).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Actual\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set\")\n",
    "\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Actual\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def regression_variation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Actual\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set\")\n",
    "\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Actual\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # # Visualize it\n",
    "    # sample = df_comp.sort_values(by='y_test').tail(40)\n",
    "    # my_range = range(1,len(sample.index)+1)\n",
    "\n",
    "    # plt.hlines(y=my_range, xmin=sample['y_pred'], xmax=sample['y_test'], color='grey')\n",
    "    # plt.scatter(sample['y_pred'], my_range, color='navy', alpha=1, label='y_pred')\n",
    "    # plt.scatter(sample['y_test'], my_range, color='gold', alpha=0.8 , label='y_test')\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.xticks(fontsize=8)\n",
    "    # plt.yticks(my_range, sample.index, fontsize=5)\n",
    "    # plt.title('Snapshot of the dataset tail showing variation between Actual\\nand Predicted measures in Differential Pressure\\n')\n",
    "    # plt.ylabel('Index No.\\n')\n",
    "    # plt.xlabel('\\nPredicted Differential Pressure vs Actual\\nAverage Variation in Pascals: %.8f' % df_comp.variance.mean())\n",
    "    # # plt.figure(figsize=(15,15))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_less_RUL = X_test.drop(labels=['RUL'], axis=1)\n",
    "regression_performance(X_train, y_train, X_test_less_RUL, y_test, best_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test_less_RUL, y_test, best_regressor_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The **R2 score** (pipeline performance) is perfect 1.0 on the **df_train** and **df_test** sets respectively\n",
    "* As the business requirement requests an R2 score of 0.7 or above, this is an exceptional result.\n",
    "\n",
    "We also note that\n",
    "* The predictions follow the actual values extremely well, (the blue dots follow along the red diagonal almost perfectly).\n",
    "● We could have added more hyperparameters in the extensive search or considered more algorithms.\n",
    "● The reason we selected fewer hyperparameter combinations in the notebook was to train all possible\n",
    "models more quickly.\n",
    "In your project you may want to consider more hyperparameters.\n",
    "● If your hyperparameter combination almost reaches your performance criteria,\n",
    "○ then you may want to add a few more hyperparameters with the expectation that it would reach the performance we stated in the business case.\n",
    "● However, in this example, as our performance is very low, we'll explore other strategies. ○ We fitted a regressor pipeline using all available data.\n",
    " \n",
    "■ However, it didn't meet our performance requirements\n",
    "● What should we do?\n",
    "● Does this mean the data doesn't have patterns to predict tenure properly for a prospect that will likely\n",
    "churn?\n",
    "● Is there any other strategy we could take, like before delivering this pipeline as our solution?\n",
    "One strategy is to replace the feature selection step for a PCA (Principal Component Analysis) step. Next to refit our ML Pipeline with a PCA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Multiple regression and classification models under consideration \n",
    "\n",
    "* sklearn.linear_model.**LinearRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "* sklearn.linear_model.**LogisticRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.predict_proba(X)*\n",
    "* sklearn.linear_model.**SGDRegressor**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.SGDClassifier()*\n",
    "\n",
    "List full of available and under consideration can be seen at scikitlearn [linear models](https://scikit-learn.org/stable/modules/linear_model.html#)\n",
    "\n",
    "* No one optimal model. the most appropriate seems .LogisticRegression()\n",
    "<!-- \n",
    "**.LinearRegression()** - Ordinary Least Squares\n",
    "**.SGDClassifier()** and **.SGDRegressor()** - Stochastic Gradient Descent - SGD\n",
    ".Ridge() \n",
    ".Lasso()\n",
    ".MultiTaskLasso()\n",
    ".ElasticNet()\n",
    ".MultiTaskElasticNet()\n",
    ".Lars() - Least Angle Regression\n",
    ".LassoLars()\n",
    ".OrthogonalMatchingPursuit() and orthogonal_mp()\n",
    ".BayesianRidge() - Bayesian Regression\n",
    ".ARDRegression() - Automatic Relevance Determination\n",
    "Generalized Linear Models\n",
    "**.LogisticRegression()** + **.predict_proba(X)**\n",
    ".TweedieRegressor()\n",
    ".Perceptron()\n",
    ".PassiveAggressiveClassifier() and .PassiveAggressiveRegressor()\n",
    "Robustness regression: outliers and modeling errors\n",
    ".RANSACRegressor()\n",
    ".TheilSenRegressor() and \n",
    ".HuberRegressor()\n",
    ".QuantileRegressor()\n",
    "Polynomial regression: extending linear models with basis functions\n",
    ".PolynomialFeatures() transformer -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with **R² score** = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train,y_train)\n",
    "\n",
    "print('Intercept :', linreg.intercept_)\n",
    "print('Coefficients :\\n', linreg.coef_)\n",
    "linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDreg = SGDRegressor()\n",
    "SGDreg.fit(X_train,y_train)\n",
    "print('Intercept :', SGDreg.intercept_)\n",
    "print('Coefficients :\\n', SGDreg.coef_)\n",
    "SGDreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "XTRreg = ExtraTreesRegressor()\n",
    "XTRreg.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFreg = RandomForestRegressor()\n",
    "RFreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DTreg = DecisionTreeRegressor()\n",
    "DTreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGBreg = XGBRegressor()\n",
    "XGBreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBreg = GradientBoostingRegressor()\n",
    "GBreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ABreg = AdaBoostRegressor()\n",
    "ABreg.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Model Evaluation\n",
    "* A good metric for this is the **coefficient of determination** also called **r2-score**.\n",
    "* A good metric for this is the **median_absolute_error**.\n",
    "* Consider Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "prediction_linear = linreg.predict(X_test)\n",
    "prediction_SGDreg = SGDreg.predict(X_test)\n",
    "prediction_XTRreg = XTRreg.predict(X_test)\n",
    "prediction_RFreg = RFreg.predict(X_test)\n",
    "prediction_DTreg = DTreg.predict(X_test)\n",
    "prediction_XGBreg = XGBreg.predict(X_test)\n",
    "prediction_GBreg = GBreg.predict(X_test)\n",
    "prediction_ABreg = ABreg.predict(X_test)\n",
    "\n",
    "print('prediction_linreg :', r2_score(y_test,prediction_linear))\n",
    "print('prediction_SGDreg :', round(r2_score(y_test,prediction_SGDreg)))\n",
    "print('prediction_XTRreg :', r2_score(y_test,prediction_XTRreg))\n",
    "print('prediction_RFreg :', r2_score(y_test,prediction_RFreg))\n",
    "print('prediction_DTreg :', r2_score(y_test,prediction_DTreg))\n",
    "print('prediction_XGBreg :', r2_score(y_test,prediction_XGBreg))\n",
    "print('prediction_GBreg :', r2_score(y_test,prediction_GBreg))\n",
    "print('prediction_ABreg :', r2_score(y_test,prediction_ABreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# prediction = linreg.predict(X_test)\n",
    "# print(classification_report(y_test,prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets \n",
    "Save the files to **/models** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/models/RULRegressor')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "grid_search_summary.to_csv(f'outputs/datasets/models/RULRegressor/GS_summary.csv',index=False)\n",
    "grid_search_pipelines.to_csv(f'outputs/datasets/models/RULRegressor/GS_pipelines.csv',index=False)\n",
    "# linreg.to_csv(f'outputs/datasets/models/RULRegressor/RUL_linreg.csv',index=False)\n",
    "# ABreg.to_csv(f'outputs/datasets/models/RULRegressor/RUL_ABreg.csv',index=False)\n",
    "\n",
    "df_total.to_csv(f'outputs/datasets/models/df_total.csv',index=False)\n",
    "df_total_model.to_csv(f'outputs/datasets/models/df_total_model.csv',index=False)\n",
    "df_train.to_csv(f'outputs/datasets/models/df_train.csv',index=False)\n",
    "df_train_even_dist.to_csv(f'outputs/datasets/models/df_train_even_dist.csv',index=False)\n",
    "df_test.to_csv(f'outputs/datasets/models/df_test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
