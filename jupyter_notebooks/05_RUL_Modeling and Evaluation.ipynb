{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "### Answer business requirement 1: \n",
        "* Using a predictive model to **determine the current Reaming Useful Life (RUL) of any given replaceable part** (in this case an industrial air filter).\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/transformed/dfTransformedTotal.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Validation set (features and target)\n",
        "* ML pipeline to predict RUL\n",
        "* Labels map\n",
        "* Feature Importance Plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_squared_error, mean_absolute_error,\n",
        "    median_absolute_error\n",
        "    )\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "\n",
        "df_total = pd.read_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv')\n",
        "frame = df_total['Data_No'].iloc[0:len(df_total)]\n",
        "df_train = df_total[frame < 51].reset_index(drop=True)\n",
        "df_test = df_total[frame > 50].reset_index(drop=True)\n",
        "df_total"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract bins that reach **600 pa** of differential pressure or more in **df_train** dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dp_total = df_train['Differential_pressure'].map(float).round(decimals=4)\n",
        "df_train['Differential_pressure'] = dp_total\n",
        "n = df_train['Differential_pressure'][0:len(df_train)]\n",
        "df_train_dp = df_train[n >= 600].reset_index(drop=True)\n",
        "RUL_extract = df_train_dp['Data_No']\n",
        "RUL_additional = df_train.loc[df_train['Data_No'].isin(RUL_extract)]\n",
        "RUL_additional"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Include **additional RUL** variables that have a fully completed test cycle to **increase the total data in the modelling dataframe**\n",
        "Remove NaN Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([df_test, RUL_additional], ignore_index=True)\n",
        "print(df_train.shape, '= df_train')\n",
        "print(df_test.shape, '= df_test')\n",
        "print(df.shape, '= df')\n",
        "df.sort_values('Data_No', ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# MP Pipeline: Regressor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Ordinal Numbers into Categorical Values\n",
        "The target and all requirements are already in a numerical format (float and integer) from our previous engineering steps. \n",
        "* **Notwithstanding**; we will convert the **dust type** back into a categorical variable to demonstrate the inclusion of a categorical encoder in each pipeline.\n",
        "* We will also take the opportunity to remove **data number** from the regression set. \n",
        "    * This variable is a category and may confound the results as each RUL measure is within a series of data bins of 'not always complete' tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_no = df['Data_No'].map(str)\n",
        "# df.drop(['Data_No'], axis=1)\n",
        "dust = df['Dust'].map(str)\n",
        "df['Dust'] = dust\n",
        "df = df.drop(['Data_No'], axis=1)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create ML pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['Dust'])),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
        "                                                        variables=['Differential_pressure', '4point_EWM', 'log_EWM',\n",
        "                                                                'Flow_rate', 'Time', 'Dust_feed',\n",
        "                                                                'change_DP', 'change_EWM', 'mass_g',\n",
        "                                                                'cumulative_mass_g', 'Tt','filter_balance'],\n",
        "                                                        method=\"spearman\",\n",
        "                                                        threshold=0.6,\n",
        "                                                        selection_method=\"variance\")),\n",
        "        (\"feat_scaling\", StandardScaler()),\n",
        "        (\"feat_selection\",  SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDmjjF3tHuCU"
      },
      "source": [
        "Custom Class for hyperparameter optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model = PipelineOptimization(self.models[key])\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring)\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score (R²)'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score (R²)': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score (R²)', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LD6B3CuhiDMT"
      },
      "source": [
        "## Split Train, Test and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pFzP2iGiIk1"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_working, X_test, y_working, y_test = train_test_split(\n",
        "    df.drop(['RUL'], axis=1),\n",
        "    df['RUL'],\n",
        "    test_size=0.25,\n",
        "    random_state=8,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(\n",
        "    X_working,\n",
        "    y_working,\n",
        "    test_size=0.25,\n",
        "    random_state=8,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print('\\n', X_train.shape, y_train.shape, '= Train set\\n',\n",
        "      X_validate.shape, y_validate.shape, '= Validation set\\n',\n",
        "      X_test.shape, y_test.shape, '= Test set\\n',\n",
        "      '===========\\n',\n",
        "      df.shape[0], '= Total Observations\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-15-sWUST6XX"
      },
      "source": [
        "## Grid Search CV - Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTFXq-ieogBj"
      },
      "source": [
        "### Use default hyperparameters to find most suitable algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZKV86gsPw8c"
      },
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
        "    \"SGDRegressor\": SGDRegressor(random_state=0),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"AdaBoostRegressor\": {},\n",
        "    \"DecisionTreeRegressor\": {},\n",
        "    \"ExtraTreesRegressor\": {},\n",
        "    \"GradientBoostingRegressor\": {},\n",
        "    'LinearRegression': {},\n",
        "    \"RandomForestRegressor\": {},\n",
        "    \"SGDRegressor\": {},\n",
        "    \"XGBRegressor\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGABtSoSLP9u"
      },
      "source": [
        "Do a hyperparameter optimisation search using default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_q-ru92GiBb"
      },
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the Color Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "cmap = mpl.colormaps['viridis']\n",
        "# cmap = plt.cm.RdBu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7p56nXeoqWo"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score (R²)')\n",
        "results = grid_search_summary['mean_score (R²)']\n",
        "results.plot(kind=\"bar\",title=\"Mean Scores (R²)\")\n",
        "\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.0,1.05])\n",
        "plt.xticks(rotation=0, fontsize=8)\n",
        "\n",
        "plt.title(\"Mean Score (R²) of various Regression Model's\\n(using all variables)\")\n",
        "plt.ylabel('Mean Score (R²)\\n')\n",
        "plt.xlabel('\\nModel Index No.')\n",
        "plt.show()\n",
        "grid_search_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average **R² score** (mean_score) indicates how well a model of the data fits the actual data. \n",
        "\n",
        "We note that\n",
        "* From the **original 6 features**, plus an additional **8 calculated** ones, produces an almost perfect prediction of remaining useful life (RUL).\n",
        "    * R² score ranges from **0.74** to **0.99**, which is exceptional, as value of 1 represents a perfect fit.\n",
        "    * This is result is exceptional, however unusual and requires further investigation.\n",
        "    * The natural inter-correlation of the calculated requirements may be influencing the models score, so we will exclude these for further review.\n",
        "* The **Random Forest Regressor** looks to be the best performing model among the 7 reviewed at this stage."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LD6B3CuhiDMT"
      },
      "source": [
        "## Exclude Calculated Requirements\n",
        "These are naturally be cross correlated to the base requirement they are calculated and may unduly skew the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consolidate columns required in **Dataframe**, **Train**, **Test** & **Validation** Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_req = ['Differential_pressure', 'Flow_rate', 'Time', 'Dust_feed', 'Dust', 'RUL']\n",
        "df = df.filter(columns_req)\n",
        "X_train = X_train.filter(columns_req)\n",
        "X_validate = X_validate.filter(columns_req)\n",
        "X_test = X_test.filter(columns_req)\n",
        "\n",
        "print('\\n', X_train.shape, y_train.shape, '= Train set\\n',\n",
        "      X_validate.shape, y_validate.shape, '= Validate set\\n',\n",
        "      X_test.shape, y_test.shape, '= Test set\\n',\n",
        "      '===========\\n',\n",
        "      df.shape[0], '= Total Observations\\n')\n",
        "      \n",
        "df.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save for Later\n",
        "We will take the opportunity to save this **hybrid dataframe** for use in the **feature study** section that looks to answer **business requirement 2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_export = df.copy()\n",
        "dust_density = ['ISO 12103-1, A2 Fine Test Dust' if n == '0.9' else ('ISO 12103-1, A3 Medium Test Dust' if n == '1.025' else 'ISO 12103-1, A4 Coarse Test Dust') for n in df_export['Dust']]\n",
        "df_export['Dust'] = dust_density\n",
        "df_export.to_csv(f'outputs/datasets/transformed/dfCombinedHybrid.csv',index=False)\n",
        "df_export.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple check to see all dust values have been converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_export.Dust.unique().reshape(-1).tolist()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Re-Define the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['Dust'])),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
        "                                                        variables=['Differential_pressure',\n",
        "                                                                'Flow_rate', 'Time', 'Dust_feed'],\n",
        "                                                        method=\"spearman\",\n",
        "                                                        threshold=0.6,\n",
        "                                                        selection_method=\"variance\")),\n",
        "        (\"feat_scaling\", StandardScaler()),\n",
        "        (\"feat_selection\",  SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-Run hyperparameter optimization search using default hyperparameters **on less variables**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score (R²)')\n",
        "results = grid_search_summary['mean_score (R²)']\n",
        "results.plot(kind=\"bar\",title=\"Mean Scores (R²)\")\n",
        "\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.xticks(rotation=0, fontsize=8)\n",
        "\n",
        "plt.title(\"Mean Score (R²) of various Regression Model's\\n(using original variables only)\")\n",
        "plt.ylabel('Mean Score (R²)\\n')\n",
        "plt.xlabel('\\nModel Index No.')\n",
        "plt.show()\n",
        "grid_search_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "* From the **original 6 features** maintains the almost perfect prediction of remaining useful life (RUL) for most regression models.\n",
        "    * R² score ranges from ±**0.45** to ±**0.95**.\n",
        "    * We see a slight reduction of the scores across all tests, which is understandable considering the removal of possibly cross correlated calculated variables.\n",
        "    * There is negligible difference between the top 3 ranked models.\n",
        "\n",
        "* The **Random Forest Regressor** remains the best performing model with an R² of **0.948985**.\n",
        "    * This is result is exceptional.\n",
        "    * High R² Scores are unusual and will require further investigation.\n",
        "\n",
        "* The R² score of the top 5 ranked estimators is much higher than the **0.7** tolerance we decided in the business case.\n",
        "    * We could use this information to feedback to the business team to review the business model.\n",
        "    * A tolerance level between **0.85** to **0.95** or higher may be suitable for this dataset / business case.\n",
        "    * At the high performance levels seen in a variety of models, the **speed of calculating each model** may also be a further consideration for the business team."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRUAeoG9lrZ"
      },
      "source": [
        "## Optimal **hyperparameter configuration** of the most suitable model\n",
        "Here we will perform an extensive grid search on the most suitable model to find the optimal combination of hyper-parameters."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First step is to define the model and parameters for the extensive search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest Regressor (12min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# documentation to help on hyperparameter list: \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "models_search = {\n",
        "    'RandomForestRegressor': RandomForestRegressor(),\n",
        "}\n",
        "\n",
        "params_search = {\n",
        "    'RandomForestRegressor':{\n",
        "        # 'model__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
        "        # 'model__criterion': ['squared_error', 'friedman_mse', 'poisson'],\n",
        "        'model__criterion': ['poisson'],\n",
        "        # # 'model__max_depth': [None],\n",
        "        # 'model__max_depth': [3,10,None],\n",
        "        'model__max_features': [1.0, 'sqrt', 'log2'],\n",
        "        # 'model__n_estimators': [100,300,600,29089],\n",
        "        'model__n_estimators': [100,400,800],\n",
        "        # 'model__n_jobs': [None, 1],\n",
        "        # 'model__n_jobs': [None],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBy8thxqAlrd"
      },
      "source": [
        "Extensive GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_4Ob7heAYM9"
      },
      "outputs": [],
      "source": [
        "search_regr = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search_regr.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNJJpLEAzdP"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search_regr.score_summary(sort_by='mean_score (R²)')\n",
        "grid_search_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extra Trees Regressor (48min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # documentation to help on hyperparameter list: \n",
        "# # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
        "\n",
        "# models_search = {\n",
        "#     'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
        "# }\n",
        "\n",
        "# params_search = {\n",
        "#     'ExtraTreesRegressor':{\n",
        "#         'model__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
        "#         # # 'model__max_depth': [None],\n",
        "#         # 'model__max_depth': [3,10,None],\n",
        "#         # 'model__max_features': [1.0, 'sqrt', 'log2'],\n",
        "#         # model__min_samples_split': [2,4,6],\n",
        "#         # 'model__n_estimators': [100,200,300],\n",
        "#         # 'model__n_jobs': [None, 1],\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extensive GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# search_et = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "# search_et.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid_search_summary_ExtraTrees, grid_search_pipelines_ExtraTrees = search_et.score_summary(sort_by='mean_score (R²)')\n",
        "# grid_search_summary_ExtraTrees"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concatenation into a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid_search_summary = pd.concat([grid_search_summary_RForest, grid_search_summary_ExtraTrees], ignore_index=True)\n",
        "# grid_search_pipelines = dict(grid_search_summary_RForest); grid_search_pipelines.update(grid_search_summary_ExtraTrees)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DWryh7BlA2df"
      },
      "source": [
        "#### Check the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVWEmpSuA4C7"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "best_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7_jvnR4sZ8km"
      },
      "source": [
        "Hyperparameters for best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2my-LZFzZ-YD"
      },
      "outputs": [],
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWXlprwaAW-"
      },
      "source": [
        "Define the best regressor, based on search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OZ24jS0aAfP"
      },
      "outputs": [],
      "source": [
        "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
        "best_regressor_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9uT2XmaKISR"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall best Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manually define these into the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_model = RandomForestRegressor(\n",
        "    criterion='poisson',\n",
        "    max_features='sqrt',\n",
        "    n_estimators=800,\n",
        "    )\n",
        "reg_model.fit(X_train, y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = (pd.Series(reg_model.feature_importances_, index=X_train.columns)\n",
        "                    .nlargest(6)\n",
        "                    .plot(kind='bar'))\n",
        "plt.xticks(fontsize=8)\n",
        "plt.title('Feature Importance\\n')\n",
        "plt.ylabel(\"F-Score\\n\")\n",
        "plt.xlabel('\\nFeature')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observations\n",
        "* From the 6 original features, we dropped `Data_No` as it is a catagorical variable that arbitrarily describes the test number and has no relation to the patterns seen in the dataset.\n",
        "* Among the remaing 5 variables, 2 show higher relevance to predict Remaining Useful Life (RUL) that the others\n",
        "    * `Dust Feed` and `Differential Pressure`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QzNyQirSKJj6"
      },
      "source": [
        "## Evaluate Regressor Performance on Train and Test Sets\n",
        "\n",
        "Compute a performance metric on the data held out for testing, **df_test**\n",
        "* [R² score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) (also called Coefficient of Determination)\n",
        "* [Mean Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE)\n",
        "* [Median Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) (MdAE)\n",
        "* [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE)\n",
        "* Root Mean Squared Error (RMSE).\n",
        "\n",
        "We could also consider:\n",
        "* Almost Correct Predictions Error Rate (ACPER)\n",
        "* Mean Absolute Percentage Error (MAPE) and \n",
        "* Adjusted R² Score \n",
        "    * _((1 - R²) * (sample_size - 1)) * -1 / (sample_size - no_independent_features - 1))_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pBm_vx8BO9s"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import (\n",
        "#     r2_score, mean_squared_error, mean_absolute_error,\n",
        "#     median_absolute_error\n",
        "#     )\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "\n",
        "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
        "    print(\"Model Evaluation \\n\")\n",
        "    print(\"* Train Set\")\n",
        "    regression_evaluation(X_train, y_train, pipeline)\n",
        "    print(\"* Test Set\")\n",
        "    regression_evaluation(X_test, y_test, pipeline)\n",
        "\n",
        "\n",
        "def regression_evaluation(X, y, pipeline):\n",
        "    prediction = pipeline.predict(X)\n",
        "    print('R² Score:', r2_score(y, prediction).round(4))\n",
        "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(4))\n",
        "    print('Median Absolute Error:', median_absolute_error(y, prediction).round(4))\n",
        "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(4))\n",
        "    print('Root Mean Squared Error:', np.sqrt(\n",
        "        mean_squared_error(y, prediction)).round(4))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
        "    pred_train = pipeline.predict(X_train)\n",
        "    pred_test = pipeline.predict(X_test)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
        "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
        "    axes[0].set_xlabel(\"Actual\")\n",
        "    axes[0].set_ylabel(\"Predictions\")\n",
        "    axes[0].set_title(\"Train Set\")\n",
        "\n",
        "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
        "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
        "    axes[1].set_xlabel(\"Actual\")\n",
        "    axes[1].set_ylabel(\"Predictions\")\n",
        "    axes[1].set_title(\"Test Set\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tV-W5nYyBPdk"
      },
      "source": [
        "Run Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgBgrKJ5KFcX"
      },
      "outputs": [],
      "source": [
        "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)\n",
        "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_regressor_pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observations\n",
        "* The pipeline performance (R² Score) Train set: ±**0.97** and Test set: ±**0.95**.\n",
        "* The represents a very high performance of the model to predict remaining useful life.\n",
        "* This is much higher than the current business requirement is an R² Score of 0.7 or higher.\n",
        "* Our hyperparameter combination exceeds our performance criteria.\n",
        "\n",
        "Additionally:\n",
        "* The predictions tend to follow the actual values.\n",
        "* We initially added more hyperparameters in the extensive search.\n",
        "* Optimal hyperparameter combinations were chosen to train all possible models more quickly.\n",
        "* We see a few outliers in the supplied dataset that tend to mirror each other, reflecting the sourcing of train, test data from the same data bins.\n",
        "\n",
        "#### Considerations\n",
        "* Due to the high performance of this model, additional hyperparameters are not warranted to increase performance in this case.\n",
        "* We could replace the **feature selection step** in the model pipeline for a **PCA (Principal Component Analysis) step** to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings).\n",
        "    * In this case, we already have a small number of attributes and performance exceeds the current business case requirement, so a PCA is not warranted, however;\n",
        "    * To **demonstrate the process** we will perform a PCA and **highlight any changes** that occur in performance.\n",
        "\n",
        "Next:\n",
        "* Refit our ML Pipeline with a PCA.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ9tjLxEIn3h"
      },
      "source": [
        "# Regressor with PCA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Review PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "pipeline = Pipeline([('scaler', MinMaxScaler()), ('regressor', RandomForestRegressor())])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "r2 = pipeline.score(X_test, y_test)\n",
        "print(f'RandomForrestRegression (defaults): {r2}') # RFR: 0.9997308011141385"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline([('scaler', MinMaxScaler()), ('regressor', RandomForestRegressor(\n",
        "    criterion='poisson',\n",
        "    max_features='sqrt',\n",
        "    n_estimators=800,\n",
        "))])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "r2 = pipeline.score(X_test, y_test)\n",
        "print(f'RandomForrestRegression (custom x 3): {r2}') # 0.999761456617107"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* All components explain ±**99%** of the data \n",
        "* Just 3 of these components also explain **99%** of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline = PipelineOptimization(model=RandomForestRegressor(random_state=0))\n",
        "# pipeline_pca = Pipeline(pipeline.steps[:4])\n",
        "# # df_pca = pipeline_pca.fit_transform(df.drop(['Data_No'], axis=1))\n",
        "# df_pca = pipeline_pca.fit_transform(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import numpy as np\n",
        "# # import seaborn as sns\n",
        "# # from sklearn.decomposition import PCA\n",
        "\n",
        "# n_components = 3\n",
        "\n",
        "# def pca_components_analysis(df_pca, n_components):\n",
        "#     pca = PCA(n_components=n_components).fit(df_pca)\n",
        "#     x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "#     ComponentsList = [\"Component \" + str(number)\n",
        "#                       for number in range(n_components)]\n",
        "#     dfExplVarRatio = pd.DataFrame(\n",
        "#         data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "#         index=ComponentsList,\n",
        "#         columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "#     dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "#     )\n",
        "\n",
        "#     PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "#     )\n",
        "\n",
        "#     print(\n",
        "#         f\"* The {n_components} components explain {round(PercentageOfDataExplained,4)}% of the data \\n\")\n",
        "#     plt.figure(figsize=(12, 5))\n",
        "#     sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "#     plt.xticks(rotation=90)\n",
        "#     plt.yticks(np.arange(0, 110, 10))\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# pca_components_analysis(df_pca=df_pca, n_components=n_components)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n_components = 2\n",
        "# pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eaMf41ZBhBk"
      },
      "source": [
        "## Rewrite ML Pipeline for Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 3\n",
        "\n",
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        \n",
        "        # (\"filter_and_split\", filter_and_split(df)),\n",
        "\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['Dust'])),\n",
        "                                                     \n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
        "                                                        variables=['Differential_pressure',\n",
        "                                                                'Flow_rate', 'Time', 'Dust_feed'],\n",
        "                                                        method=\"spearman\",\n",
        "                                                        threshold=0.6,\n",
        "                                                        selection_method=\"variance\")),\n",
        "        (\"feat_scaling\", StandardScaler()),\n",
        "        # PCA replace Feature Selection\n",
        "        # (\"feat_selection\",  SelectFromModel(model)),\n",
        "        (\"PCA\", PCA(n_components=n_components, random_state=0)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irUsq475Bn7N"
      },
      "source": [
        "## Grid Search CV – Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Summary:\\n', X_train.shape, y_train.shape, '= Train set\\n', X_test.shape, y_test.shape, '= Test set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LVF-KR_Bqum"
      },
      "source": [
        "### Use standard hyperparameters to find the most suitable model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XmJNoUcJkKX"
      },
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
        "    \"SGDRegressor\": SGDRegressor(random_state=0),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"AdaBoostRegressor\": {},\n",
        "    \"DecisionTreeRegressor\": {},\n",
        "    \"ExtraTreesRegressor\": {},\n",
        "    \"GradientBoostingRegressor\": {},\n",
        "    'LinearRegression': {},\n",
        "    \"RandomForestRegressor\": {},\n",
        "    \"SGDRegressor\": {},\n",
        "    \"XGBRegressor\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq8td65fJkKY"
      },
      "source": [
        "Do a quick optimisation search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1BdqEB6JkKZ"
      },
      "outputs": [],
      "source": [
        "quick_search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "quick_search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ_Xj5oGJkKZ"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIC2csxKJkKZ"
      },
      "outputs": [],
      "source": [
        "grid_search_summary_PCA, grid_search_pipelines_PCA = quick_search.score_summary(sort_by='mean_score (R²)')\n",
        "grid_search_summary_PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383vOhJZBwza"
      },
      "source": [
        "### Do an extensive search on the most suitable model to find the best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcbiQHlB9QT"
      },
      "source": [
        "Define model and parameters for extensive search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7t-fum6B9QU"
      },
      "outputs": [],
      "source": [
        "# documentation to help on hyperparameter list: \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "models_search = {\n",
        "    'RandomForestRegressor': RandomForestRegressor(),\n",
        "}\n",
        "\n",
        "params_search = {\n",
        "    'RandomForestRegressor':{\n",
        "        # 'model__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
        "        # 'model__criterion': ['squared_error', 'friedman_mse', 'poisson'],\n",
        "        'model__criterion': ['poisson'],\n",
        "        # # 'model__max_depth': [None],\n",
        "        # 'model__max_depth': [3,10,None],\n",
        "        'model__max_features': [1.0, 'sqrt', 'log2'],\n",
        "        # 'model__n_estimators': [100,300,600,29089],\n",
        "        'model__n_estimators': [100,400,800],\n",
        "        # 'model__n_jobs': [None, 1],\n",
        "        # 'model__n_jobs': [None],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvt-IOmHB9QU"
      },
      "source": [
        "Extensive GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXbTwW1UB9QV"
      },
      "outputs": [],
      "source": [
        "search_PCA = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search_PCA.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVUDzRSGB9QV"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZfOdh5kB9QW"
      },
      "outputs": [],
      "source": [
        "grid_search_summary_PCA, grid_search_pipelines_PCA = search_PCA.score_summary(sort_by='mean_score (R²)')\n",
        "grid_search_summary_PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgoLo5C8B9QW"
      },
      "source": [
        "Check the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3UGYjpcB9QW"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search_summary_PCA.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjY2MdBNB9QX"
      },
      "source": [
        "Parameters for best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uspihv71B9QX"
      },
      "outputs": [],
      "source": [
        "grid_search_pipelines_PCA[best_model].best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8HonhniB9QX"
      },
      "source": [
        "Define the best regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC8U4skKB9QY"
      },
      "outputs": [],
      "source": [
        "best_regressor_pipeline = grid_search_pipelines_PCA[best_model].best_estimator_\n",
        "best_regressor_pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_model = RandomForestRegressor(\n",
        "    criterion='poisson',\n",
        "    max_features='sqrt',\n",
        "    n_estimators=800,\n",
        "    )\n",
        "reg_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = (pd.Series(reg_model.feature_importances_, index=X_train.columns)\n",
        "                    .nlargest(6)\n",
        "                    .plot(kind='bar'))\n",
        "plt.xticks(fontsize=8)\n",
        "plt.title('Feature Importance\\n')\n",
        "plt.ylabel(\"F-Score\\n\")\n",
        "plt.xlabel('\\nFeature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKGmSgINCQwj"
      },
      "source": [
        "## Evaluate Regressor on Train and Tests Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78tt_ZkiJRdE"
      },
      "outputs": [],
      "source": [
        "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)\n",
        "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ3u0TodDdOZ"
      },
      "source": [
        "# Which pipeline to choose?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FE5va8Cr-CCy"
      },
      "source": [
        "We fitted the following pipelines:\n",
        "* Random Forest Regression (with all variables)\n",
        "* Random Forest Regression (with original 6 variables)\n",
        "* Random Forest Regression with PCA\n",
        "<!-- * Classifier -->"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sQR54xeCbIAH"
      },
      "source": [
        "### Observations\n",
        "All the regressor pipelines exceeded the expected performance threshold (0.7 R² score) for the train and test set.\n",
        "The \n",
        "\n",
        "The Importance of features changed between Regression vs Regression + PCA processes:\n",
        "* 3 pipeline components explain more than 90% of the data and improves the performance of the model.\n",
        "* The `max_features` component changes from `log3` to `sqrt` improving all measures of performance.\n",
        "* The `n_components` component changed from `800` to `400` improving all measures of performance.\n",
        "* The R² Score is moderately improved.\n",
        "* Error rates significantly decrease across both **train** and **test** sets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|| Performance Measure | Regressor | Regressor + PCA |\n",
        "|---|---|---|---|\n",
        "|**Train Set**|R² Score:|± 0.97|± 0.99|\n",
        "||Mean Absolute Error:|± 6.66|± 1.75|\n",
        "||Median Absolute Error:|± 3.70|± 0.71|\n",
        "||Mean Squared Error:|± 111.80|± 9.62|\n",
        "||Root Mean Squared Error:|± 10.57|± 3.10|\n",
        "|||||\n",
        "|**Test Set**|R² Score:|± 0.95|± 0.98|\n",
        "||Mean Absolute Error:|± 9.49|± 4.46|\n",
        "||Median Absolute Error:|± 5.48|± 1.81|\n",
        "||Mean Squared Error:|± 227.77|± 61.71|\n",
        "||Root Mean Squared Error:|± 15.09|± 7.86|"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Future Features**:\n",
        "* Future models may be tuned was tuned on Remaining Useful Life accross 3 Dust classes.\n",
        "    * This may assist us to detect the remaining useful life relative to the size of dust particles for business case 2\n",
        "<!-- * It has strong performance for class A4 (<4 months) and class A2 (+20 months) -->\n",
        "<!-- * It has reasonable performance for class A2 (<4 months) and class A2 (+20 months) -->\n",
        "<!-- * It has weak performance for class A3 (<4 months) and class A2 (+20 months) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline_clf\n",
        "best_regressor_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BqT1Kne54Fq"
      },
      "source": [
        "# Refit pipeline with best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 2\n",
        "\n",
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        # (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "        #                                              variables=['Dust'])),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
        "                                                        variables=['Differential_pressure','Dust_feed', 'Time'],\n",
        "                                                        method=\"spearman\",\n",
        "                                                        threshold=0.6,\n",
        "                                                        selection_method=\"variance\")),\n",
        "        (\"feat_scaling\", StandardScaler()),\n",
        "         # (\"feat_selection\",  SelectFromModel(model)),\n",
        "        (\"PCA\", PCA(n_components=n_components, random_state=0)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GpgS-AgU6IWx"
      },
      "source": [
        "## Consolidate Dataset, Train, Test and Validation Set, only with best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_req = ['Differential_pressure', 'Flow_rate', 'Time', 'Dust_feed', 'Dust']\n",
        "df = df.filter(columns_req)\n",
        "X_train = X_train.filter(columns_req)\n",
        "X_validate = X_validate.filter(columns_req)\n",
        "X_test = X_test.filter(columns_req)\n",
        "\n",
        "print('\\n', X_train.shape, y_train.shape, '= Train set\\n',\n",
        "      X_validate.shape, y_validate.shape, '= Validate set\\n',\n",
        "      X_test.shape, y_test.shape, '= Test set\\n',\n",
        "      '===========\\n',\n",
        "      df.shape[0], '= Total Observations\\n')\n",
        "      \n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohPWfCs2E_3G"
      },
      "source": [
        "Subset Best Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_regressor_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_model = RandomForestRegressor(\n",
        "    criterion='poisson',\n",
        "    max_features='sqrt',\n",
        "    n_estimators=800,\n",
        "    )\n",
        "reg_model.fit(X_train, y_train)\n",
        "feat_importances = (pd.Series(reg_model.feature_importances_, index=X_train.columns)\n",
        "                    .nlargest(6)\n",
        "                    .plot(kind='bar'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consolidate **Train**, **Test** and **Validation** data by top 3 feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUEIfyLU6IWz"
      },
      "outputs": [],
      "source": [
        "n_features = 3\n",
        "best_features = pd.Series(reg_model.feature_importances_, index=X_train.columns).nlargest(n_features).index.to_list()\n",
        "X_train = X_train.filter(best_features)\n",
        "X_test = X_test.filter(best_features)\n",
        "X_validate = X_validate.filter(best_features)\n",
        "\n",
        "print('Summary:\\n', X_train.shape, y_train.shape,'= Train set\\n',\n",
        "        X_validate.shape, y_validate.shape, '= Validation set\\n',\n",
        "        X_test.shape, y_test.shape, '= Test set\\n')\n",
        "X_train.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fT_mdLWFJFz"
      },
      "source": [
        "## Grid Search CV – Sklearn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 2\n",
        "\n",
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['Dust'])),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(\n",
        "                                                        # variables=['Differential_pressure','Dust_feed', 'Time'],\n",
        "                                                        variables=['Differential_pressure','Dust_feed'],\n",
        "                                                        method=\"spearman\",\n",
        "                                                        threshold=0.6,\n",
        "                                                        selection_method=\"variance\")),\n",
        "        (\"feat_scaling\", StandardScaler()),\n",
        "        #  (\"feat_selection\",  SelectFromModel(model)),\n",
        "        (\"PCA\", PCA(n_components=n_components, random_state=0)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfKEBTyLeDtj"
      },
      "source": [
        "We are using the same model from the previous GridCV search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1qcZktreHH5"
      },
      "outputs": [],
      "source": [
        "models_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WaSA9jcecXr"
      },
      "source": [
        "And the best parameters from the previous GridCV search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXi0L025eKA6"
      },
      "outputs": [],
      "source": [
        "best_parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_7jAkvlBeeQl"
      },
      "source": [
        "Include manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9HBXI2E58_5"
      },
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    'RandomForestRegressor': RandomForestRegressor(),\n",
        "}\n",
        "\n",
        "params_search = {\n",
        "    'RandomForestRegressor':{\n",
        "        'model__criterion': ['poisson'],\n",
        "        'model__max_features': ['sqrt'],\n",
        "        'model__n_estimators': [800],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEZYXLRQfvTL"
      },
      "source": [
        "GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcgDvuLRfwsE"
      },
      "source": [
        "\n",
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loZEVp8g6q9O"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score (R²)')\n",
        "grid_search_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TE6Xgvif1ek"
      },
      "source": [
        "Check the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf6qYXV06q9O"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeB08Md3f60p"
      },
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuA9mpyk6q9P"
      },
      "outputs": [],
      "source": [
        "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
        "best_regressor_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN0aj0iv6q9P"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN-blGZb6q9P"
      },
      "outputs": [],
      "source": [
        "reg_model = RandomForestRegressor(\n",
        "    criterion='poisson',\n",
        "    max_features='sqrt',\n",
        "    n_estimators=800,\n",
        "    )\n",
        "reg_model.fit(X_train, y_train)\n",
        "df_feature_importance = pd.Series(reg_model.feature_importances_, index=X_train.columns).nlargest(n_features).index.to_list()\n",
        "print(f'The {len(best_features)} most important features in descending order. \\n'\n",
        "      f'\\nThe above model was trained on the following variables: \\n{df_feature_importance}')\n",
        "feat_importances = (pd.Series(reg_model.feature_importances_, index=X_train.columns)\n",
        "                    .nlargest(6)\n",
        "                    .plot(kind='bar'))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GBtppR73G1Yx"
      },
      "source": [
        "# Save files to the repo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuJ5tYUC06o"
      },
      "source": [
        "We will generate the following files to include in the app\n",
        "\n",
        "* Train set\n",
        "* Test set\n",
        "* Validation set\n",
        "* Modeling pipeline\n",
        "* Features importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vBpPvnaG5Mb"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v2'\n",
        "file_path = f'outputs/ml_pipeline/predict_rul/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TvoMsi3DNw1"
      },
      "source": [
        "## Train Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJHmwyqgDOr1"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh6w6R7tDOvM"
      },
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB6pjmAcDOym"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ93HN6cDPBN"
      },
      "outputs": [],
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbS3OnRDYtJ"
      },
      "source": [
        "## Test Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgF38n1DaPp"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9lM0xDvDaVZ"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jz66iMaDacI"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weYaJ4UxDake"
      },
      "outputs": [],
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgF38n1DaPp"
      },
      "outputs": [],
      "source": [
        "X_validate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9lM0xDvDaVZ"
      },
      "outputs": [],
      "source": [
        "X_validate.to_csv(f\"{file_path}/X_validate.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weYaJ4UxDake"
      },
      "outputs": [],
      "source": [
        "y_validate.to_csv(f\"{file_path}/y_validate.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-XpkYAPFncu"
      },
      "source": [
        "## Modelling pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xLmFFWF6RGo6"
      },
      "source": [
        "ML pipeline for predicting RUL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQkr4rcrHDnn"
      },
      "outputs": [],
      "source": [
        "best_regressor_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrZPif2aHdyO"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=best_regressor_pipeline, filename=f\"{file_path}/RandomForestRegressor_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTJlYRC5Q2wJ"
      },
      "source": [
        "## Feature importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = (pd.Series(reg_model.feature_importances_, index=X_train.columns)\n",
        "                    .nlargest(6)\n",
        "                    .plot(kind='bar'))\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh0SKfv_s-3V"
      },
      "source": [
        "C'est Fini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Predict Tenure.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
