{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
        "## **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and save it as raw data.\n",
        "* Inspect the data and save it under outputs/datasets/collection\n",
        "\n",
        "## Inputs\n",
        "\n",
        "*   Kaggle JSON file - the authentication token.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generates Two Datasets: \n",
        "    1. outputs/datasets/collection/**PredictiveMaintenanceTest**.csv\n",
        "    2. outputs/datasets/collection/**PredictiveMaintenanceTrain**.csv\n",
        "\n",
        "## Additional Comments\n",
        "* The data is from a publicly accessible Kaggle repo found [here](https://www.kaggle.com/datasets/prognosticshse/preventive-to-predicitve-maintenance) and comes pre-divided into distinctly different Testing and Training data.\n",
        "* For the purposes of the learning context of this project, we are hosting the data in a publicly accessible repo at [GitHub](https://github.com/roeszler/filter-maintenance-predictor).\n",
        "* In the workplace, we would never push data to a public repository due to security exposure it represents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The notebooks are stored in a subfolder. When running the notebook in the editor, we change the working directory from its current folder to its parent folder.\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "To make the parent of the current directory the new current directory\n",
        "* `os.path.dirname()` = gets the parent directory\n",
        "* `os.chir()` = defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"Current directory set to new location\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Fetch data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install an instance of Kaggle to work within the editor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- We have pre-installed Kaggle package to fetch data using : \n",
        "\n",
        "`pip install kaggle==1.5.12` -->\n",
        "\n",
        "This can be pre included in the requirements.txt documentation to load on initialization using : \n",
        "\n",
        "`pip3 freeze --local > requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Download a .JSON file (authentication token) from Kaggle and include it in the root directory\n",
        "* kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Recognize the token in the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Define the Kaggle dataset, and destination folder and download it.\n",
        "\n",
        "Kaggle url: [/prognosticshse/preventive-to-predicitve-maintenance](https://www.kaggle.com/datasets/prognosticshse/preventive-to-predicitve-maintenance) .\n",
        "* **Note** the misspelling of 'predictive'\n",
        "\n",
        "The following function: \n",
        "* Retrieves the Kaggle dataset\n",
        "* Creates a destination folder folder for the data to be placed\n",
        "* Downloads it to the destination folder\n",
        "* Unzips the downloaded file\n",
        "* Deletes the **.zip** file and unused data\n",
        "* Removes any **kaggle.json** files used to access the dataset on Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KaggleDatasetPath = 'prognosticshse/preventive-to-predicitve-maintenance'\n",
        "DestinationFolder = 'inputs/datasets/raw'   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n",
        "\n",
        "! unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
        "  && rm {DestinationFolder}/*.zip \\\n",
        "  && rm {DestinationFolder}/*.pdf_test_test_test_test_test_test_test \\\n",
        "  && rm {DestinationFolder}/*.mat \\\n",
        "#   && rm kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load and Inspect Kaggle data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Data to Inspect\n",
        "We could combine both datasets, however as they have been included as two sets with slightly different content, we will inspect them each separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_test_test = pd.read_csv(f'inputs/datasets/raw/Test_Data_CSV.csv')\n",
        "df_train = pd.read_csv(f'inputs/datasets/raw/Train_Data_CSV.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DataFrame Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre installed `pandas_profiling` and `ipywidgets` with: \n",
        "\n",
        "* `pip install pandas-profiling`\n",
        "\n",
        "* `pip install ipywidgets`\n",
        "\n",
        "Not forgetting to update the requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pandas-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### To explore the **Test** dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report_test = ProfileReport(df=df_test, minimal=True)\n",
        "pandas_report_test.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main observations of the **Test** Dataset :\n",
        "\n",
        "* There are no missing cells.\n",
        "\n",
        "* Differential Pressure has zero's and has a **reverse exponential** shaped distribution \n",
        "    * This correlates to what we understand. The beginning of each test set will have a period where the filter is clean and the difference in pressure is negligible.\n",
        "    * Subsequently, the measures of distribution (Mean, Median, Mode, Skewness, Kurtosis) correlate to the reverse exponential shape\n",
        "\n",
        "* Most of the **Dust_Feed** was run at 60mm<sup>3</sup>/s\n",
        "    * possibly manipulate data to make the range of test sets more evenly distributed\n",
        "\n",
        "* There is more than three times the amount of A3 Medium Dust observations (47.9%) as there is A2 Fine dust (14.8%), with A4 Course tests (37.3%)\n",
        "    * possibly manipulate data to make the range of test sets more evenly distributed\n",
        "    \n",
        "* The RUL target distribution is right or **positively skewed** at 0.71.\n",
        "    * Confirmed by the **Mean** of **111.48** > **Median** of **93.5**\n",
        "    * An ideal normal distribution has mean, median and mode similar in value and a skewness measure approaching 0\n",
        "    * A measure of the distributions tails; Kurtosis at -0.34 is relatively low in value and negative, indicating few outliers \n",
        "    * Further box plot visualization to further investigate this skewness.\n",
        "    * We will consider manipulating data at the feature engineering stage to reduce the affect of skewness, like:\n",
        "        * Random Forest Selection (Bagging)\n",
        "        * Logarithmic transformation\n",
        "        * Manipulate the data range to that of test sets more evenly distributed\n",
        "        * Feature Scaling\n",
        "\n",
        "#### Note: \n",
        "This dataset has deliberately had the tails of its observations removed at random points (right censored). This needs to be considered when looking at engineering the distributions of this dataset. In light of this and further Principal Component Analysis (PCA) a Random Forest Selection (Bagging) may present itself as the preferred method to engineer this set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.kurtosis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### To explore the **Train** dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report_train = ProfileReport(df=df_train, minimal=True)\n",
        "pandas_report_train.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What group do the zeros appear in mostly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main observations of the **Train** Dataset :\n",
        "\n",
        "* There are also no missing cells.\n",
        "\n",
        "* Differential Pressure has zero's and has the same **reverse exponential** shaped distribution as df_test.\n",
        "    * This correlates to what we understand. The beginning of each test set will have a period where the filter is clean and the difference in pressure is negligible.\n",
        "    * Subsequently, the measures of distribution (Mean, Median, Mode, Skewness, Kurtosis) correlate to the same reverse exponential shape\n",
        "\n",
        "* Most of the **Dust_Feed** was ab bit more evenly spread through the data, a 27% from 158.5mm<sup>3</sup>/s to around 20% in feeds between 60mm<sup>3</sup>/s to 118mm<sup>3</sup>/s.\n",
        "    * In a live project, we would check the stakeholders as to possible reasons for this and confirm that it represents typical data seen in practice\n",
        "    * possibly manipulate data to make the range of test sets more evenly distributed\n",
        "\n",
        "* The dust observations maintain A3 Medium Dust as the highest proportion (47.9%), however the portions of A2 Fine dust (28.2%) to A4 Course Dust (23.7%) are approximately the same.\n",
        "    * We would also check this with the stakeholders in a live workplace project\n",
        "    * possibly manipulate data to make the range of test sets more evenly distributed\n",
        "\n",
        "#### Reminder Note: \n",
        "This dataset has deliberately had the tails of its observations removed at random points (right censored). This needs to be considered when looking at engineering the distributions of this dataset. In light of this and further Principal Component Analysis (PCA) a Random Forest Selection (Bagging) may present itself as the preferred method to engineer this set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We note that the dataset has **no missing data**. \n",
        "* This is outside of what we already know to be true for **df_test** (with RUL) and **df_train** (without RUL)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extend Data_No of df_test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A comparison between sets reveals that the **Data_No** variable:\n",
        "* Is a categorical variable presented as an integer\n",
        "* Restarts at the beginning of each dataset\n",
        "\n",
        "This has the potential to confound subsequent analysis between the sets, where the analysis erroneously considers *Data_No* a discrete value &/or a duplicate entry. To help avoid confusion we alter the values in the **df_test dataset** to be a continuation from the bins seen in the **df_train dataset**.\n",
        "\n",
        "This is as simple as adding the total number of unique test bins in the df_test set to each one seen in the df_train set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick reminder of the tables we are working with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the total number of test sets in **df_train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Continue the numbering in the next set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace new data references into **df_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check new Data_No values in both sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Combine datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combined_list = [df_test, df_train, df_validate]\n",
        "# df = pd.concat(combined_list)\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save the files to an output folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df_train.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceTrain.csv',index=False)\n",
        "df_test.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceTest.csv',index=False)\n",
        "# df_validate.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceValidate.csv',index=False)\n",
        "# df.to_csv(f'outputs/datasets/collection/FilterMaintenancePredictorDataset.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusions: \n",
        "* Data supplied without missing observations\n",
        "* The Data_No references were repeated and corrected\n",
        "\n",
        "#### Next Steps:\n",
        "* Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
