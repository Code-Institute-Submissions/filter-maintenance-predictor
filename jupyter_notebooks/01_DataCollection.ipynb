{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
        "## **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and save it as raw data.\n",
        "* Inspect the data and save it under outputs/datasets/collection\n",
        "\n",
        "## Inputs\n",
        "\n",
        "*   Kaggle JSON file - the authentication token.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generates Two Datasets: \n",
        "    1. outputs/datasets/collection/**PredictiveMaintenanceTest**.csv\n",
        "    2. outputs/datasets/collection/**PredictiveMaintenanceTrain**.csv\n",
        "\n",
        "## Additional Comments\n",
        "* The data is from a publicly accessible Kaggle repo found [here](https://www.kaggle.com/datasets/prognosticshse/preventive-to-predicitve-maintenance) and comes pre-divided into distinctly different Testing and Training data.\n",
        "* For the purposes of the learning context of this project, we are hosting the data in a publicly accessible repo at [GitHub](https://github.com/roeszler/filter-maintenance-predictor).\n",
        "* In the workplace, we would never push data to a public repository due to security exposure it represents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The notebooks are stored in a subfolder. When running the notebook in the editor, we change the working directory from its current folder to its parent folder.\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "To make the parent of the current directory the new current directory\n",
        "* `os.path.dirname()` = gets the parent directory\n",
        "* `os.chir()` = defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Fetch data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We have pre-installed Kaggle package to fetch data using : \n",
        "`pip install kaggle==1.5.12`\n",
        "\n",
        "#### This is pre included in the requirements.txt documentation to load on initialization using : \n",
        "\n",
        "`pip3 freeze --local > requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Download a .JSON file (authentication token) from Kaggle and include it in the root directory\n",
        "* kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Recognize the token in the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Define the Kaggle dataset, and destination folder and download it.\n",
        "\n",
        "Kaggle url: [/prognosticshse/preventive-to-predicitve-maintenance](https://www.kaggle.com/datasets/prognosticshse/preventive-to-predicitve-maintenance) .\n",
        "* **Note** the misspelling of 'predictive'\n",
        "\n",
        "The following function: \n",
        "* Retrieves the Kaggle dataset\n",
        "* Creates a destination folder folder for the data to be placed\n",
        "* Downloads it to the destination folder\n",
        "* Unzips the downloaded file\n",
        "* Deletes the **.zip** file and unused data\n",
        "* Removes any **kaggle.json** files used to access the dataset on Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KaggleDatasetPath = 'prognosticshse/preventive-to-predicitve-maintenance'\n",
        "DestinationFolder = 'inputs/datasets/raw'   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n",
        "\n",
        "! unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
        "  && rm {DestinationFolder}/*.zip \\\n",
        "  && rm {DestinationFolder}/*.pdf \\\n",
        "  && rm {DestinationFolder}/*.mat \\\n",
        "#   && rm kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load and Inspect Kaggle data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Data to Inspect\n",
        "We could combine both datasets, however as they have been included as two sets with slightly different content, we will inspect them each separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_test = pd.read_csv(f'inputs/datasets/raw/Test_Data_CSV.csv')\n",
        "df_train = pd.read_csv(f'inputs/datasets/raw/Train_Data_CSV.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DataFrame Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre installed `pandas_profiling` and `ipywidgets` with: \n",
        "\n",
        "* `pip install pandas-profiling`\n",
        "\n",
        "* `pip install ipywidgets`\n",
        "\n",
        "Not forgetting to update the requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### To explore the **Test** dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report_test = ProfileReport(df=df_test, minimal=True)\n",
        "pandas_report_test.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### To explore the **Train** dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report_train = ProfileReport(df=df_test, minimal=True)\n",
        "pandas_report_train.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### To view the distributions of the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We note that the dataset has **no missing data**. \n",
        "* This is outside of what we already know to be true for **df_test** (with RUL) and **df_train** (without RUL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extend Data_No of df_test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A comparison between sets reveals that the **Data_No** variable:\n",
        "* Is a categorical variable presented as an integer\n",
        "* Restarts at the beginning of each dataset\n",
        "\n",
        "This has the potential to confound subsequent analysis between the sets, where the analysis erroneously considers *Data_No* a discrete value &/or a duplicate entry. To help avoid confusion we alter the values in the **df_test dataset** to be a continuation from the bins seen in the **df_train dataset**.\n",
        "\n",
        "This is as simple as adding the total number of unique test bins in the df_test set to each one seen in the df_train set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick reminder of the tables we are working with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the total number of test sets in **df_train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Continue the numbering in the next set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace new data references into **df_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check new Data_No values in both sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Combine datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combined_list = [df_test, df_train, df_validate]\n",
        "# df = pd.concat(combined_list)\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save the files to an output folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df_train.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceTrain.csv',index=False)\n",
        "df_test.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceTest.csv',index=False)\n",
        "# df_validate.to_csv(f'outputs/datasets/collection/PredictiveMaintenanceValidate.csv',index=False)\n",
        "# df.to_csv(f'outputs/datasets/collection/FilterMaintenancePredictorDataset.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions and Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusions: \n",
        "* Data supplied without missing observations\n",
        "* The Data_No references were repeated and corrected\n",
        "\n",
        "#### Next Steps:\n",
        "* Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Dec  2 2022, 16:09:02) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
