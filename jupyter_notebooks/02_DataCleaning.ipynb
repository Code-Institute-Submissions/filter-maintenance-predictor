{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **Data Cleaning Notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Confirm / Evaluate missing data\n",
    "*   Clean data in preparation for analysis\n",
    "\n",
    "### Inputs\n",
    "\n",
    "1. Test Dataset : `outputs/datasets/collection/PredictiveMaintenanceTest.csv`\n",
    "\n",
    "2. Train Dataset : `outputs/datasets/collection/PredictiveMaintenanceTrain.csv`\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Generate cleaned Train and Test sets, both saved under `outputs/datasets/cleaned`\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "  * Data Cleaning Pipeline\n",
    "  * Drop Variables as Required\n",
    "  <!-- `['customerID', 'TotalCharges' ]` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Collection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(f'outputs/datasets/collection/PredictiveMaintenanceTrain.csv')\n",
    "df_test = pd.read_csv(f'outputs/datasets/collection/PredictiveMaintenanceTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm we don't have variables with missing data, and if we do; discover their distribution and shape.\n",
    "* Note: we are aware that the **df_train** dataset does not have values for `RUL`, so both sets are checked separately\n",
    "\n",
    "If we tried to combine the sets to check, it would indicate `RUL` has missing values like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_train, df_test])\n",
    "vars_with_missing_data = df_total.columns[df_total.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To check both datasets for missing data at the same time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a handy function to identify which dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_dataframe(data):\n",
    "    \"\"\" To identify which dataframe is being accessed \"\"\"\n",
    "    name =[n for n in globals() if globals()[n] is data][0]\n",
    "    print('Dataframe name: %s' % name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data & return error information if there is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "for df in (df_train, df_test):\n",
    "    vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "    if vars_with_missing_data:\n",
    "        profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "        profile.to_notebook_iframe()\n",
    "    else:\n",
    "        name_dataframe(df)\n",
    "        print('There are no variables with missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evenly distribute dataset by `Dust` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the train and test sets supplied have data distributed unevenly between 50 test bins. To account for this we wish to assess the measures of central tendency for each Dust class, with tha aim of reducing the data size to a more evenly proportioned one between classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider % `censored` calculation to all observations in both datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train** Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations**\n",
    "\n",
    "* The proportion of data that **has reached filter failure**. These may be worth keeping and will make part of our heuristic decision process.\n",
    "* The **mean** is the most frequently used measure of central tendency because it uses all values in the data set to give you an average.\n",
    "* For data from skewed distributions (like `differential_pressure`), the **median** is better than the mean because it isnâ€™t influenced by extremely large values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the top ten `Data_No` bins where `differential_pressure` observations that have made it to the **600 Pa** (the point of filter failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_train = df_train[df_train.Data_No != df_train.Data_No.shift(-1)]\n",
    "# last_row_descending = last_row_train.sort_values(by='Dust', ascending=True)\n",
    "last_row_descending = last_row_train.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Dust` variable in this dataset shows a disproportionate mix between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Central Tendency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next aim is to \n",
    "* Make the size of each bin around **9400** observations (similar to the A4 Coarse Dust class bin) \n",
    "* Fill these bins with data that best represents a central tendency.\n",
    "\n",
    "#### Procedure\n",
    "* Include a comparison to how far each `differential_pressure` measure **deviates** or how far it is from the **.median()** value of the bin.\n",
    "* Create a dataframe of the A3 Medium Dust : **1.025**\n",
    "* Ordered by `filter_balance` showing sets with data closest to 600 Pa `differential_pressure`.\n",
    "* Include comparison to median\n",
    "* And a cumulative measure of Data_Nos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Standard Deviation to **df_train** test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_group = df_train.groupby('Data_No').std()\n",
    "std_group.index.name = None\n",
    "std_group['Data_No'] = std_group.index\n",
    "map_std = df_train['Data_No'].map(std_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train['std_DP'] = map_std\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the calculations\n",
    "* ref `std_DP` values for data_no 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A3_1 = df_train[df_train['Data_No'] == 1].std()\n",
    "dust_A3_1['Differential_pressure']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Median to **df_train**\n",
    "* Note: Median is the preferred measure of central tendency to observe in a skewed dataset such as this as it is not as affected by larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_group = df_train.groupby('Data_No').median()\n",
    "median_group.index.name = None\n",
    "median_group['Data_No'] = median_group.index\n",
    "map_median = df_train['Data_No'].map(median_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train['median_DP'] = map_median\n",
    "df_train.loc[446:452]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the calculations\n",
    "* ref `median_DP` values for data_no 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A3_1 = df_train[df_train['Data_No'] == 1].median()\n",
    "dust_A3_1['Differential_pressure']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the size of each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_size = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "# bin_size.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_Tot'])\n",
    "df_train['bin_Size'] = map_bin\n",
    "# df_train.loc[38817:38827]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the dataframe with just **A3 Dust** in it, ordered by `filter_balance`\n",
    "* This includes a cumulative sum of each **bin size** and **cumulative sum** of it. This will help us decide on the data bin that reaches **9400** or more total values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3 = filter_A3.sort_values(by='Filter_Balance', ascending=True)\n",
    "df_train_A3['c_Sum'] = df_train_A3['bin_Size'].cumsum()\n",
    "df_train_A3.head(13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the current dataframe containing only A3 Medium Dust observations, that is ordered by those tests with closest to a completed test to failure:\n",
    "* The top 12 data bins (seen at bin 21) would extract a A3 Medium dust training dataset with **9,764 observations**\n",
    "* We will now perform a further PDA to evaluate the suitability of these further"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract these bins from the df_train dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a frame of the bin numbers we wish to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A3['Data_No'].head(12)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the bins to eventually **.append() to our final cleaned dataset\n",
    "* Note we disregard the cumulative sum measure as it doesn't add value to further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A3 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI: we can create dataframe with data_ref values removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_less_bins = df_train_copy[~df_train_copy['Data_No'].isin(data_ref)]\n",
    "df_train_less_bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape we started with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A3.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned_A3.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat tests that used A2 Fine Dust  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_A3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_A3.median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through each data bin in this dust class and calculate the .median() values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.std()\n",
    "df_train.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df_train[df_train['Data_No'] == '12']\n",
    "# df_bin.median().round(decimals=2)\n",
    "df_bin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How far the measure is from the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract each class and compare distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_test = df_test[df_test.Data_No != df_test.Data_No.shift(-1)]\n",
    "last_row_descending = last_row_test.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataframe distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (df_train, df_test):\n",
    "    df_numpy = df\n",
    "    df_numpy.to_numpy()\n",
    "    name_dataframe(df_numpy)\n",
    "    print(df_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Power Predictive Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the files to /cleaned folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "df_train.to_csv(f'outputs/datasets/cleaned/dfCleanTrain.csv',index=False)\n",
    "df_test.to_csv(f'outputs/datasets/cleaned/dfCleanTest.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "* \n",
    "\n",
    "#### Next Steps:\n",
    "* Correlation Study\n",
    "* Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Dec  2 2022, 16:09:02) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
