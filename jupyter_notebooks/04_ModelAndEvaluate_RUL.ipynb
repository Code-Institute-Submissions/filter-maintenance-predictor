{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **ML Model - Predict Remaining Useful Life (RUL)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Answer [Business Requirement 1](https://github.com/roeszler/filter-maintenance-predictor/blob/main/README.md#business-requirements) :\n",
    "*   Fit and evaluate a **regression model** to predict the Remaining Useful Life of a replaceable part\n",
    "*   Fit and evaluate a **classification model** to predict the Remaining Useful Life of a replaceable part should the regressor not perform well.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Data cleaning:\n",
    "* outputs/datasets/cleaned/dfCleanTotal.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Validation set (features and target)\n",
    "* ML pipeline to predict RUL\n",
    "* A map of the labels\n",
    "* Feature Importance Plot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The major steps in this Regressor Pipeline\n",
    "\n",
    "1. **ML Pipeline: Regressor**\n",
    "    * Create Regressor Pipeline\n",
    "    * Split the train set\n",
    "    * Grid Search CV SKLearn\n",
    "        * Use standard hyperparameters to find most suitable algorithm\n",
    "        * Extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "    * Assess Feature Performance\n",
    "    * Evaluate Regressor\n",
    "    * Create Train, Test, Validation Sets\n",
    "\n",
    "2. **ML Pipeline: Regressor + Principal Component Analysis (PCA)**\n",
    "    * Prepare the Data for the Pipeline\n",
    "    * Create Regressor + PCA Pipeline\n",
    "    * Split the train and validation sets\n",
    "    * Grid Search CV SKLearn\n",
    "        * Use standard hyperparameters to find most suitable algorithm\n",
    "        * Do an extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "    * Assess Feature Performance\n",
    "    * Evaluate Regressor\n",
    "    * Create Train, Test, Validation Sets\n",
    "\n",
    "_Optionally_\n",
    "\n",
    "3. **Convert Regression to Classification**\n",
    "    * Convert numerical target to bins, and check if it is balanced\n",
    "    * Rewrite Pipeline for ML Modelling\n",
    "    * Load Algorithms For Classification\n",
    "    * Split the Train Test sets:\n",
    "    * Grid Search CV SKLearn:\n",
    "        * Use standard hyper parameters to find most suitable model\n",
    "        * Grid Search CV\n",
    "        * Check Result\n",
    "    * Do an extensive search on the most suitable model to find the best hyperparameter configuration.\n",
    "        * Define Model Parameters\n",
    "        * Extensive Grid Search CV                             \n",
    "        * Check Results\n",
    "        * Check Best Model\n",
    "        * Parameters for best model\n",
    "        * Define the best clf_pipeline\n",
    "    * Assess Feature Importance\n",
    "    * Evaluate Classifier on Train and Test Sets\n",
    "        * Custom Function\n",
    "        * List that relates the classes and tenure interval\n",
    "\n",
    "4. **Decide which pipeline to use**\n",
    "\n",
    "5. **Refit with the best features**\n",
    "    * Rewrite Pipeline\n",
    "    * Split Train Test Set with only best features\n",
    "    * Subset best features\n",
    "    * Grid Search CV SKLearn\n",
    "    * Best Parameters\n",
    "        * Manually\n",
    "    * Grid Search CV\n",
    "    * Check Results\n",
    "    * Check Best Model\n",
    "    * Define the best pipeline\n",
    "\n",
    "6. **Assess Feature Importance**\n",
    "\n",
    "7. **Push Files to Repo**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Modelling:\n",
    "The hypothesis part of the process where you will find out whether you can answer the question.\n",
    "* Identify what techniques to use.\n",
    "* Split your data into train, validate and test sets.\n",
    "* Build and train the models with the train data set.\n",
    "* Validate Models and hyper-parameter : Trial different machine learning methods and models with the validation data set.\n",
    "* Poor Results - return to data preparation for feature engineering\n",
    "* Successful hypothesis - where the inputs from the data set are mapped to the output target / label appropriately to evaluate.\n",
    "\n",
    "5. Evaluation:\n",
    "Where you test whether the model can predict unseen data.\n",
    "* Test Dataset\n",
    "* Choose the model that meets the business success criteria best.\n",
    "* Review and document the work that you have done.\n",
    "* If your project meets the success metrics you defined with your customer?\n",
    "- Ready to deploy. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data\n",
    "The pipeline should handle the cleaning and engineering by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "# ! chmod 600 kaggle.json\n",
    "\n",
    "# KaggleDatasetPath = 'prognosticshse/preventive-to-predicitve-maintenance'\n",
    "# DestinationFolder = 'inputs/datasets/raw'   \n",
    "# ! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n",
    "\n",
    "# ! unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
    "#   && rm {DestinationFolder}/*.zip \\\n",
    "#   && rm {DestinationFolder}/*.pdf \\\n",
    "#   && rm {DestinationFolder}/*.mat \\\n",
    "# #   && rm kaggle.json\n",
    "\n",
    "# df_test = pd.read_csv(f'inputs/datasets/raw/Test_Data_CSV.csv')\n",
    "# df_train = pd.read_csv(f'inputs/datasets/raw/Train_Data_CSV.csv')\n",
    "# # df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "# df_total = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "# df_total = pd.read_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv') # data with negative log_EWM values removed\n",
    "# df_total = (pd.read_csv(\"outputs/datasets/collection/PredictiveMaintenanceTotal.csv\").drop(labels=['customerID', 'TotalCharges', 'Churn'], axis=1))\n",
    "print(df_total.shape)\n",
    "# df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include 4 Point Exponentially Weighted Mean and Remove Negative Values\n",
    "\n",
    "**Note:** A runtime warning indicating a divide by zero is expected in the calculation of the log_EWM. We are using this to identify which values to delete, so have temporarily suppressed the warning for this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Log EWM to identify values to remove\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_means = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        ewm_calc = df_bin['Differential_pressure'].ewm(span=4, adjust=False).mean()\n",
    "        df_bin.insert(loc=2, column='4point_EWM', value=ewm_calc)\n",
    "\n",
    "        log_ewm = np.log(ewm_calc)\n",
    "        df_bin.insert(loc=3, column='log_EWM', value=log_ewm)\n",
    "\n",
    "        df_means = pd.concat([df_means, df_bin], ignore_index = True)\n",
    "df_total = df_means\n",
    "\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# Delete Negatives\n",
    "data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)].reset_index(drop=True)\n",
    "\n",
    "# Delete engineered values\n",
    "del df_total['log_EWM']\n",
    "# del df_total['4point_EWM']\n",
    "\n",
    "# df_total.loc[391:397]\n",
    "# print(df_total.shape)\n",
    "df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Transformations\n",
    "* Numerical Transformation as required\n",
    "* Smart correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
    "            encoding_method='arbitrary', variables=['gender', 'Partner', 'Dependents',\n",
    "            'PhoneService','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "            'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',\n",
    "            'PaperlessBilling', 'PaymentMethod'])\n",
    "        ),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        (\"model\", model),])\n",
    "    return pipeline_base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into Train, Test, Validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was continuous with discrete observations:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= ad_data.drop(['Ad Topic Line', 'City', 'Timestamp', 'Clicked on Ad', 'Country'],axis=1)\n",
    "y = ad_data['Clicked on Ad']\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=42)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is discrete however in bins, so:\n",
    "#### Re-Extract Cleaned **Train** & **Test** Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitData(df):\n",
    "#     n = df['Data_No'].iloc[0:len(df)]\n",
    "#     df_train = pd.DataFrame(df[n < 51].reset_index(drop=True))\n",
    "#     df_test = pd.DataFrame(df[n > 50].reset_index(drop=True))\n",
    "#     # df_train = df[n < 51].reset_index(drop=True)\n",
    "#     # df_test = df[n > 50].reset_index(drop=True)\n",
    "#     del df_train['RUL']\n",
    "#     return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitData(df_total)\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_total['Data_No'].iloc[0:len(df_total)]\n",
    "df_train = df_total[n < 51].reset_index(drop=True)\n",
    "df_test = df_total[n > 50].reset_index(drop=True)\n",
    "del df_train['RUL']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineer Even Distributions in **Train** and **Test** Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine **Validation** Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine **Target** and **Independent** Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= ad_data.drop(['Ad Topic Line', 'City', 'Timestamp', 'Clicked on Ad', 'Country'],axis=1)\n",
    "y = ad_data['Clicked on Ad']\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Multiple regression and classification models under consideration \n",
    "\n",
    "* sklearn.linear_model.**LinearRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "* sklearn.linear_model.**LogisticRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.predict_proba(X)*\n",
    "* sklearn.linear_model.**SGDRegressor**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.SGDClassifier()*\n",
    "\n",
    "List full of available and under consideration can be seen at scikitlearn [linear models](https://scikit-learn.org/stable/modules/linear_model.html#)\n",
    "\n",
    "* No one optimal model. the most appropriate seems .LogisticRegression()\n",
    "<!-- \n",
    "**.LinearRegression()** - Ordinary Least Squares\n",
    "**.SGDClassifier()** and **.SGDRegressor()** - Stochastic Gradient Descent - SGD\n",
    ".Ridge() \n",
    ".Lasso()\n",
    ".MultiTaskLasso()\n",
    ".ElasticNet()\n",
    ".MultiTaskElasticNet()\n",
    ".Lars() - Least Angle Regression\n",
    ".LassoLars()\n",
    ".OrthogonalMatchingPursuit() and orthogonal_mp()\n",
    ".BayesianRidge() - Bayesian Regression\n",
    ".ARDRegression() - Automatic Relevance Determination\n",
    "Generalized Linear Models\n",
    "**.LogisticRegression()** + **.predict_proba(X)**\n",
    ".TweedieRegressor()\n",
    ".Perceptron()\n",
    ".PassiveAggressiveClassifier() and .PassiveAggressiveRegressor()\n",
    "Robustness regression: outliers and modeling errors\n",
    ".RANSACRegressor()\n",
    ".TheilSenRegressor() and \n",
    ".HuberRegressor()\n",
    ".QuantileRegressor()\n",
    "Polynomial regression: extending linear models with basis functions\n",
    ".PolynomialFeatures() transformer -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDreg = SGDRegressor()\n",
    "SGDreg.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction = logrig.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
