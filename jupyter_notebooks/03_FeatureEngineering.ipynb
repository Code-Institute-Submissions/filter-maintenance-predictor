{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **Feature Engineering Notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Cleaning**\n",
    "\n",
    "Performed within the [data cleaning notebook](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/02_DataCleaning.ipynb).\n",
    "\n",
    "**2. Data Transformation**\n",
    "* Processing the data for the modelling stage.\n",
    "* Transform data into a format that is useful for the algorithm learn the relationship among the variables.\n",
    "* Evaluate the use of the following approaches to engineer the variables:\n",
    "    * ordinal categorical encoding\n",
    "    * numerical transformation\n",
    "    * smart correlated selection\n",
    "    \n",
    "**3. Feature Extraction**\n",
    "* Evenly distribute dust type\n",
    "\n",
    "**4. Feature Selection**\n",
    "\n",
    "**5. Feature Iteration**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inputs\n",
    "\n",
    "1. Cleaned Test Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "2. Cleaned Train Dataset : `outputs/datasets/collection/dfCleanTrain.csv`\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Generate engineered Train and Test sets, both saved under `outputs/datasets/transformed`\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "  * Best approach to engineer variables based on...\n",
    "  * Transformations that we will consider in our pipeline are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Categorical encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert `Data_No` to a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_no_total = df_total['Data_No'].map(str)\n",
    "# df_total['Data_No'] = data_no_total\n",
    "# df_total.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical transformations\n",
    "\n",
    "This process can consider transformers like:\n",
    "* Smooth with SMA\n",
    "* Logarithmic in base e\n",
    "* Logarithmic in base 10\n",
    "* Reciprocal\n",
    "* Power\n",
    "* BoxCox\n",
    "* Yeo Johnson\n",
    "\n",
    "Note: Part of the challenge with this data is dealing with a continuous dataset that is comprised of data 'bins'. Each `Data_No` data bin represents an individual test cycle. \n",
    "\n",
    "When engineering **descriptive statistics** into the dataset, that typically used a sample of the previous values to indicate a mean or standard deviation, **we need to treat each bin individually** and not progress the first row of each bin incorrectly with the last rows of the previous bin.\n",
    "\n",
    "To solve this, we use a sequential loop that proceeds through each bin and inserts a progressive descriptive statistic calculation like change in differential pressure (`change_DP`) and appends it to the previous bin. This eventually creates a version of the dataset that includes the new calculation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in Differential Pressure\n",
    "Include change in Differential Pressure calculation\n",
    "\n",
    "**Note**: We replace first instance of `change_DP` with first value of `differential_pressure` in the new bin. \n",
    "* This signifies that the fist observation starts from a **zero** DP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change_dp = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        change_dp_calc = df_bin['Differential_pressure'].diff().fillna(df_bin['Differential_pressure'])\n",
    "        df_bin.insert(loc=7, column='change_DP', value=change_dp_calc)\n",
    "\n",
    "        df_change_dp = pd.concat([df_change_dp, df_bin], ignore_index = True)\n",
    "df_total = df_change_dp\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in differential pressure observations\n",
    "\n",
    "In each bin we notice that the size and direction of the `change_DP` measure occasionally produces a zero or negative value. This highlights a fluctuation in the **differential pressures**. These may be considered outliers as the pressure gradient across the filter needs time to stabilize. We have considered three main methods to deal with these observations:\n",
    "* Log transformation\n",
    "* Winsorize method\n",
    "* Dropping the outliers\n",
    "\n",
    "These will be handled in the [feature engineering](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/03_FeatureEngineering.ipynb) notebook\n",
    "\n",
    "**Random sample** (Bin 96) to plot and inspect change in Differential Pressure distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "bin_96 = df_total[df_total['Data_No'] == 96]\n",
    "sns.boxplot(x = bin_96['change_DP'], ax=ax1)\n",
    "sns.histplot(x = bin_96['change_DP'], ax=ax2)\n",
    "sns.lineplot(x=bin_96['Time'], y=bin_96['Differential_pressure'], ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing of **Differential Pressure**\n",
    "*Dealing with outliers in differential pressure observations*\n",
    "\n",
    "In each bin note `differential_pressure` and `change_DP` observations. Occasionally the measures fluctuate outside of the general trend of the data (outliers). This can be seen below in the **second to last** observation of Data_No bin 98 (index no. 75826) where the recorded value is outside the general trend of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total['Data_No'] == 98].tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify such measures, have considered a tolerance of Â±200% change in value, however this can be altered depending on what we see when fitting the models.\n",
    "\n",
    "To **smooth** the variability of this measure we can apply a **mean** (or average) value to the data in various ways. \n",
    "This attempt to soften the severity of changes seen and reduce the instances of values that are higher / lower than the general trend. It will effectively reduce variability in the **differential pressure** measure, making it easier to model.\n",
    "\n",
    "We have considered the following methods to deal with outliers:\n",
    "* Smooth with Simple Mean Average (SMA)\n",
    "* Exponentially Weighted Mean (EWM)\n",
    "* Log transformation\n",
    "* Dropping the outliers\n",
    "* Winsorize method\n",
    "\n",
    "For each calculation that uses previous observations to produce a value, we can use the same process we applied to manage the unique data bins as we did in calculating **change in differential pressure** above.\n",
    "\n",
    "#### Smoothing of Differential Pressure with a **Simple Moving Average** (SMA)\n",
    "A simple moving average (SMA) is an arithmetic **moving average** calculated by adding recent values, then dividing that by the number of observations in the calculation. In this case we used the same process to manage the data bins to include SMA of the last four measures, that moves as the observations progress.\n",
    "\n",
    "* We can see the first four observations of the SMA are **NaN** indicated. These could be imputed with arbitrary values, mean values, closest k sample values and/or a MICE (Multiple Imputation by Chained Equations) value that fits a linear regression with the present values.\n",
    "    * On review, the MICE method would be the preferred method to impute the missing SMA values, however considering the progressive nature of the `differential_pressure` variable, a preferable alternate to SMA would be Exponentially Weighted Mean (EWM).\n",
    "\n",
    "#### Smoothing of Differential Pressure with an **Exponentially Weighted Mean** (EWM)\n",
    "Is a measure of the moving average that considers older observations to have given lower weightings in the calculation. The weights fall exponentially as the data point gets older â hence the name exponentially weighted.\n",
    "\n",
    "* The EWM is calculated with a 4 point EWM (`span=4`) measure, that considers the previous 4 observations to calculate the weighted mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        ewm_calc = df_bin['Differential_pressure'].ewm(span=4, adjust=False).mean()\n",
    "        df_bin.insert(loc=2, column='4point_EWM', value=ewm_calc)\n",
    "\n",
    "        sma_calc = df_bin['Differential_pressure'].rolling(4).mean()\n",
    "        df_bin.insert(loc=2, column='4point_SMA', value=sma_calc)\n",
    "\n",
    "        change_ewm_calc = df_bin['4point_EWM'].diff().fillna(df_bin['4point_EWM'])\n",
    "        df_bin.insert(loc=10, column='change_EWM', value=change_ewm_calc)\n",
    "\n",
    "        df_means = pd.concat([df_means, df_bin], ignore_index = True)\n",
    "df_total = df_means\n",
    "df_total.loc[446:451]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logarithmic Transformation** of Differential Pressure\n",
    "\n",
    "As seen at data collection, the original continuous `differential_pressure` data is right or **positively skewed** at 1.81 and does not follow the shape of a normally distributed bell curve.\n",
    "* Included a plot of the **smoothed** variable to check that it is representative of the **differential_pressure** source, which it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "sns.histplot(x = df_total['Differential_pressure'], ax=ax1)\n",
    "sns.histplot(x = df_total['4point_SMA'], ax=ax2)\n",
    "sns.histplot(x = df_total['4point_EWM'], ax=ax3)\n",
    "\n",
    "ax1.title.set_text('Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Simple Moving Average Histogram\\n')\n",
    "ax3.title.set_text('Exponential Weighted Mean Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **log transformation** of this data will represent the values within a normal distribution, **as much as possible**, allowing a more valid statistical analysis from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,6))\n",
    "\n",
    "df_total_log_dp = df_total['Differential_pressure']\n",
    "log_dp = np.log(df_total_log_dp)\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "\n",
    "df_total_log_sma = df_total['4point_SMA']\n",
    "log_sma = np.log(df_total_log_sma)\n",
    "sns.histplot(x = log_sma, ax=ax2)\n",
    "\n",
    "df_total_log_ewm = df_total['4point_EWM']\n",
    "log_ewm = np.log(df_total_log_ewm)\n",
    "sns.histplot(x = log_ewm, ax=ax3)\n",
    "\n",
    "# plt.title('Log Histogram')\n",
    "ax1.title.set_text('Log_DP Histogram\\n')\n",
    "ax2.title.set_text('Log_SMA Histogram\\n')\n",
    "ax3.title.set_text('Log_EWM Histogram\\n')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** \n",
    "* The shape of the numerical differential_pressure data is improved by a natural logarithmic transformation\n",
    "    * Untreated values of 78.23 mean, std.deviation at 107.34 and median at 35.17, indicates the positively skewed nature of the original data\n",
    "    * Transformed the data is much improved with mean 3.18 much closer to the median 3.89\n",
    "\n",
    "The transformed data is still affected by the negative skew in the data:\n",
    "* Pressure measures show unusual behavior at the start of most tests, producing an unusual entry tail of values at zero or below.\n",
    "    * These can be managed by \n",
    "        * adding a constant to stop the value becoming negative, or \n",
    "        * indicate the negative as a missing number or \n",
    "        * dropping the entire observation\n",
    "    * Considering these measures indicate a zone at the beginning of the test procedure to 'equalize' the difference in pressure between the areas before and after the filter, we can treat them as outliers and confidently drop the entire row this observation sits in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further EWM transformation**\n",
    "\n",
    "Remove rows with negative numbers in `4pointWEM` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "# data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "# df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "# # df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Negative values\n",
    "df_total.insert(loc=4, column='log_EWM', value=log_ewm)\n",
    "data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)]\n",
    "\n",
    "# Visualize the data\n",
    "old_shape = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "sns.histplot(x = log_dp, ax=ax1)\n",
    "sns.histplot(x = df_total['log_EWM'], ax=ax2)\n",
    "\n",
    "plt.title('log_EWM Histogram')\n",
    "ax1.title.set_text('Transformed Differential Pressure Histogram\\n')\n",
    "ax2.title.set_text('Transformed Differential Pressure Exponentially Weighted Mean Histogram (Values > 0)\\n')\n",
    "plt.show()\n",
    "print(\"Old Shape: \", old_shape.shape)\n",
    "print(\"New Shape: \", df_total.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformed data is far more useable to train the model. \n",
    "* Less variability \n",
    "* Negative values removed\n",
    "* Minimal loss of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Dropping of Outliers\n",
    "Dropping all negative numbers to reduce the noise of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of detecting outliers using **Inter Quartile Range** (IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_IQR = df_total.copy()\n",
    "\n",
    "# IQR\n",
    "Q1 = np.percentile(df_total_IQR, 25, method='midpoint')\n",
    "Q3 = np.percentile(df_total_IQR, 75,method='midpoint')\n",
    "IQR = Q3 - Q1\n",
    "print(\"Old Shape: \", df_total_IQR.shape)\n",
    " \n",
    "# Upper bound\n",
    "upper = np.where(df_total_IQR >= (Q3+1.5*IQR))\n",
    "# Lower bound\n",
    "lower = np.where(df_total_IQR <= (Q1-1.5*IQR))\n",
    " \n",
    "''' Removing the Outliers '''\n",
    "df_total_IQR.drop(upper[0], inplace = True)\n",
    "df_total_IQR.drop(lower[0], inplace = True)\n",
    " \n",
    "print(\"New Shape: \", df_total_IQR.shape)\n",
    "\n",
    "# df_total_log_IQR = df_total_IQR['Differential_pressure']\n",
    "# df_total_log_IQR = df_total_IQR['4point_SMA']\n",
    "# df_total_log_IQR = df_total_IQR['4point_EWM']\n",
    "df_total_log_IQR = df_total_IQR['log_EWM']\n",
    "log_dp_IQR = np.log(df_total_log_IQR)\n",
    "# df_total_IQR['log_DP'] = log_dp\n",
    "sns.histplot(x = df_total_log_IQR)\n",
    "plt.title('IQR Histogram of log EWM')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR transformation is not particularly effective on this data with negative values removed. We will not apply this method and proceed with other techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Winsorize Method**\n",
    "Winsorization is the process of replacing the extreme values of statistical data in order to limit the effect of the outliers on the calculations or the results obtained by using that data. To apply this to a our exponentially logged differential pressure `log_EWM` variable, where outliers are present only at one end of the data:\n",
    "* The lower 10% values of the data will have their values set equal to the value of the data point at the 10th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "WinsorizedArrayMean = np.mean(df_total['log_EWM'])\n",
    "winz_EWM = winsorize(df_total['log_EWM'],(0.1,0.1))\n",
    "df_total.insert(loc=5, column='winz_EWM', value=winz_EWM)\n",
    "WinsorizedArray = df_total['winz_EWM']\n",
    "plt.boxplot(WinsorizedArray)\n",
    "plt.title('Winsorized array')\n",
    "plt.show()\n",
    "WinsorizedArrayNewMean = np.mean(WinsorizedArray)\n",
    "print('Old Mean: ', WinsorizedArrayMean)\n",
    "print('New Mean: ', WinsorizedArrayNewMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_log_winz = df_total['winz_EWM']\n",
    "log_winz = np.log(df_total_log_winz)\n",
    "sns.histplot(x = log_winz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winzorization smooths the data too much and does not add much value to training our models so we will not apply this method either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_total['winz_EWM']\n",
    "del df_total['4point_SMA']\n",
    "df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evenly distribute dataset by `Dust` type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the **train** and **test** sets supplied have data distributed unevenly between 50 test bins. To account for this we wish to assess the measures of central tendency for each Dust class, with the aim of reducing the data size to a more evenly proportioned one between classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train** Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations**\n",
    "\n",
    "* The proportion of data that **has reached filter failure** is represented by how close `filter_balance` is to zero or less. \n",
    "    * Data with filter_balance values approaching zero may be worth keeping and will make part of our heuristic decision process.\n",
    "* Notwithstanding that the **mean** is the most frequently used measure of central tendency because it uses all values in the data set to give you an average\n",
    "    * For data from skewed distributions (like `differential_pressure`), the **median** is better than the mean because it isnât influenced by extremely large values.\n",
    "\n",
    "In the following calculation, we see a summary of the top ten `Data_No` bins where `differential_pressure` observations that have made it to the **600 Pa** (the point of filter failure)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide back into **Train** and **Test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row = df_total[df_total.Data_No != df_total.Data_No.shift(-1)]\n",
    "# last_row_descending = last_row.sort_values(by='Dust', ascending=True)\n",
    "last_row_descending = last_row.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_train = df_train[df_train.Data_No != df_train.Data_No.shift(-1)]\n",
    "# last_row_descending = last_row_train.sort_values(by='Dust', ascending=True)\n",
    "last_row_descending = last_row_train.sort_values(by='Differential_pressure', ascending=False)\n",
    "last_row_descending.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the diagram below showing proportions of `Dust` variable in the **df_train** dataset.\n",
    "* It shows a disproportionate mix between classes. This will be the first dataset we tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next aim is to \n",
    "* Fill these bins with data that best represents a central tendency.\n",
    "* Make the size of each bin around Â±**5000** observations (similar to the A4 Coarse Dust class bin) \n",
    "\n",
    "#### Procedure\n",
    "* Include a comparison to how far each `differential_pressure` measure **deviates** or how far it is from the **.median()** value of the bin.\n",
    "* Ordered by `filter_balance` showing sets with data closest to 600 Pa `differential_pressure`.\n",
    "* Include comparison to median\n",
    "* Add a cumulative measure of Data_No's to use as a ranking\n",
    "* Create a dataframe of the A3 Medium Dust : **1.025**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Standard Deviation to **df_train** test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['winz_Mean']\n",
    "del df_train['log_DP']\n",
    "std_group = df_train.groupby('Data_No').std()\n",
    "std_group.index.name = None\n",
    "std_group.loc[:,'Data_No'] = std_group.index\n",
    "map_std = df_train['Data_No'].map(std_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'std_DP'] = map_std\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[444:453]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculation of Coefficient of Variation (variance) to **df_train** test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cv = lambda data: np.std(data, ddof=1) / np.mean(data, axis=0) * 100 \n",
    "var_group = df_train.groupby('Data_No').apply(cv)\n",
    "var_group.index.name = None\n",
    "var_group.loc[:,'Data_No'] = var_group.index\n",
    "map_var = df_train['Data_No'].map(var_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'cv_DP'] = map_var\n",
    "# df_test.loc[363:368]\n",
    "df_train.loc[444:453]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of variation is an indication of how far the standard deviation is away from the mean. As we can see it does not add value to our understanding of the data, primarily due the the skewed nature of the `differential_pressure` continuous variable. \n",
    "* This re-enforces the understanding that descriptive statistics using the mean may not be preferred measure of central tendency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Coefficient of Variation and Add Median to df_train**\n",
    "* Median is the preferred measure of central tendency to observe in a skewed dataset such as this as it is not as affected by larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['cv_DP']\n",
    "median_group = df_train.groupby('Data_No').median()\n",
    "median_group.index.name = None\n",
    "median_group.loc[:,'Data_No'] = median_group.index\n",
    "map_median = df_train['Data_No'].map(median_group.set_index('Data_No')['Differential_pressure'])\n",
    "df_train.loc[:,'median_DP'] = map_median\n",
    "df_train.loc[444:453]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can evaluate the dataframe with just **A3 Dust** in it, ordered by `filter_balance` as a measure of time to filter failure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the size of each bin and include a cumulative sum of each **bin size** to help see which data bin that reaches **4500** or more total values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_Tot'])\n",
    "df_train.loc[:, 'bin_Size'] = map_bin\n",
    "# df_train.loc[38817:38827]\n",
    "dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3 = filter_A3.sort_values(by='Filter_Balance', ascending=True)\n",
    "df_train_A3['c_Sum'] = df_train_A3['bin_Size'].cumsum()\n",
    "df_train_A3.head(13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the current dataframe containing only A3 Medium Dust observations, that is ordered by those tests with closest to a completed test to failure:\n",
    "* **The top 9 data bins (seen at bin 7) would extract a A3 Medium dust training dataset with 5,109 observations**\n",
    "* We will now perform a further PDA to evaluate the suitability of these further"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by Standard Deviations, ordered by `std_DP`\n",
    "The standard deviation is used to measure the spread of values in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_std = filter_A3.sort_values(by='std_DP', ascending=True)\n",
    "df_train_A3_std['c_Sum'] = df_train_A3_std['bin_Size'].cumsum()\n",
    "df_train_A3_std.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank by central tenancy of the Median value, ordered by `median_DP`\n",
    "* the value of the number in the middle of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dust_A3 = df_train[df_train['Dust'] == 1.025]\n",
    "# filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3_median = filter_A3.sort_values(by='median_DP', ascending=True)\n",
    "df_train_A3_median['c_Sum'] = df_train_A3_median['bin_Size'].cumsum()\n",
    "df_train_A3_median.head(14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations\n",
    "* T..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract these bins from the df_train dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a separate frame indicating the bin numbers we wish to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A3['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these references to create a dataframe `df_train_cleaned_A3` that is ready for inclusion in our final dataframe `df_train_clean`.\n",
    "* Note we disregard the cumulative sum measure as it doesn't add value to further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A3 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Quick Review: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape started with: ', dust_A3.shape)\n",
    "print('Shape we have now: ', df_train_cleaned_A3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned_A3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these to a new dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train_compare\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat procedure with remaining `Dust` classes and **Test** dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract, Clear and Replace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_sum = df_train.groupby('Data_No')['Data_No'].count().reset_index(name='bin_Tot')\n",
    "# map_bin = df_train['Data_No'].map(bin_sum.set_index('Data_No')['bin_Tot'])\n",
    "# df_train.loc[:, 'bin_Size'] = map_bin\n",
    "\n",
    "# df_train.loc[38817:38827]\n",
    "\n",
    "dust_A2 = df_train[df_train['Dust'] == 0.900]\n",
    "filter_A2 = dust_A2[dust_A2.Data_No != dust_A2.Data_No.shift(-1)]\n",
    "df_train_A2 = filter_A2.sort_values(by='Filter_Balance', ascending=True)\n",
    "df_train_A2['c_Sum'] = df_train_A2['bin_Size'].cumsum()\n",
    "df_train_A2.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_no = df_train_A2['Data_No'].head(9)\n",
    "bin_no.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train\n",
    "df_train_cleaned_A2 = df_train_copy[df_train_copy['Data_No'].isin(bin_no)]\n",
    "df_train_cleaned_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_A2 = df_train_cleaned_A2\n",
    "dust_A3 = df_train_cleaned_A3\n",
    "dust_A4 = df_train[df_train['Dust'] == 1.200]\n",
    "\n",
    "df_train_compare = pd.concat([dust_A2, dust_A3, dust_A4], ignore_index = True)\n",
    "df_train_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "category_totals = df_train_compare.groupby('Dust')['Differential_pressure'].count().sort_values()\n",
    "category_totals.plot(kind=\"barh\", title='Proportion of Dust Classes in df_train_compare\\n', xlabel='\\nObservations', ylabel='Dust Class')\n",
    "category_totals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much data do we need?\n",
    "* At a bin sample of 9 for both A2 and A3 dust, we currently have a little under 15,000 observations to train the ML Models\n",
    "* Will review this depending on the performance of the models\n",
    "* Overfitting with too much data vs underfitting with too little is a balance and the qualitative nature of the heuristics surrounding the predictions made\n",
    "* Nonlinear Algorithms (like clustering) may need more data\n",
    "\n",
    "Short answer is, **we have plenty of extra data at this point that may be less suited to training the model**. \n",
    "\n",
    "Notwithstanding, it is still live data and may there may be some value toward increasing the power of our ML Models. \n",
    "We do have the option to alter the selected number of dust class bins and possibly augmenting the smaller A4 dust dataset with the **Synthetic Minority Oversampling Technique (SMOTE)** or SMOTE NC for categorical data should we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train_compare\n",
    "df_train_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide **df_total** into original **df_test** & **df_train** proportions as supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
    "df_total['Data_No'] = data_no_total\n",
    "n = df_total['Data_No'][0:len(df_total)]\n",
    "df_train = df_total[n < 51].reset_index(drop=True, names='index')\n",
    "df_test = df_total[n > 50].reset_index(drop=True, names='index')\n",
    "del df_train['RUL']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets \n",
    "Save the files to /transformed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/transformed')\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "df_train.to_csv(f'outputs/datasets/transformed/dfTransformedTrain.csv',index=False)\n",
    "df_test.to_csv(f'outputs/datasets/transformed/dfTransformedTest.csv',index=False)\n",
    "df_total.to_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "* \n",
    "\n",
    "#### Next Steps:\n",
    "* Regression Model\n",
    "* Classification Model\n",
    "* Cluster Model\n",
    "* Correlation Study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
